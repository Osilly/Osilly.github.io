<html>

<head>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
    <title>Wenxuan Huang (ÈªÑÊñáËΩ©)</title>
    <meta content="Wenxuan Huang (ÈªÑÊñáËΩ©), Osilly.github.io" name="keywords" />
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            /* font-family: Lato, sans-serif; */
            font-family: Arial;
            /* font-family: Georgia; */
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #043d98;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }



        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 2em auto 2em auto;
            width: 900px;
            font-family: Open Sans Light, Helvetica, sans-serif;
            font-size: 15px;
            background: #F4F6F6;
            height: 100%;
        }

        h2 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            /* font-family: Arial, Verdana, Helvetica, sans-serif; */
            /* font-size: 13px; */
            font-weight: bold;
        }

        ul {
            /* list-style: circle; */
            list-style: disc;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Arial, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.2em;
            background: #F4F6F6;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.4em;
            margin-bottom: 0.7em;
            border: 2px solid #ddd;
            background: #fff;
            padding: 0.55em .8em 0.6em .8em;
            border-top-right-radius: 10px;
            border-top-left-radius: 10px;
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
            line-height: 140%;
        }

        div.paper2 {
            clear: both;
            margin-top: 0.4em;
            margin-bottom: 0.7em;
            border: 0px solid #ddd;
            background: #fff;
            padding: 0.55em .8em 0.6em .8em;
            border-top-right-radius: 10px;
            border-top-left-radius: 10px;
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
            line-height: 140%;
        }

        div.experience {
            /* clear: both; */
            margin-top: 0.4em;
            margin-bottom: 0.7em;
            /* border: 2px solid #ddd; */
            background: #fff;
            padding: 0.55em .8em 0.6em .8em;
            /* border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px; */
            line-height: 140%;
        }


        div.education {
            /* clear: both; */
            margin-top: 0.4em;
            margin-bottom: 0.7em;
            /* border: 2px solid #ddd; */
            background: #fff;
            padding: 0.55em .8em 0.6em .8em;
            /* border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px; */
            line-height: 140%;
        }

        div.snap {
            /* clear: both; */
            margin-top: 0.4em;
            margin-bottom: 0.7em;
            /* border: 2px solid #ddd; */
            background: #fff;
            padding: 0.55em .8em 0.6em .8em;
            /* border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px; */
            line-height: 140%;
        }

        div.paper:hover {
            background: #FFFDEE;
            /* background-color: #242d36 ; */
        }

        div.paper2:hover {
            background: #FFFDEE;
            /* background-color: #242d36 ; */
        }

        div.bio {
            clear: both;
            margin-top: 0.4em;
            margin-bottom: 0.7em;
            border: 0px solid #ddd;
            background: #fff;
            padding: 0.55em .8em 0.6em .8em;
            border-top-right-radius: 10px;
            border-top-left-radius: 10px;
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
            line-height: 135%;
        }

        div.res {
            clear: both;
            margin-top: 0.4em;
            margin-bottom: 0.4em;
            border: 0px solid #ddd;
            background: #fff;
            padding: 0.65em .8em 0.15em .8em;
            border-top-right-radius: 10px;
            border-top-left-radius: 10px;
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
            line-height: 130%;
        }

        div.award {
            clear: both;
            margin-top: 0.4em;
            margin-bottom: 0.4em;
            border: 0px solid #ddd;
            background: #fff;
            padding: 0.65em .8em 0.15em .8em;
            border-top-right-radius: 10px;
            border-top-left-radius: 10px;
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
            line-height: 130%;
        }

        div.paper div {
            padding-left: 270px;
        }

        div.experience div {
            padding-left: 200px;
        }

        div.education div {
            padding-left: 200px;
        }

        img.paper {
            /* margin-bottom: 0.4em; */
            float: left;
            width: 250px;

        }

        img.experience {
            /* margin-bottom: 0.4em; */
            padding-top: 10px;
            float: left;
            width: 170px;

        }

        img.education {
            /* margin-bottom: 0.4em; */
            padding-left: 30px;
            float: left;
            width: 100px;

        }

        img.snap {
            /* margin-bottom: 0.4em; */
            padding-left: 40px;
            float: left;
            width: 88px;

        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: Open Sans Light, Helvetica, sans-serif;
            font-size: 14px;
            margin: 1em 0;
            padding: 0;
        }

        .bot {
            font-size: 14%;
        }

        .ptypej {
            display: inline;
            padding: .0em .2em .05em;
            font-size: 85%;
            font-weight: bold;
            line-height: 1;
            background-color: #5cb85c;
            color: #FFFFFF;
            text-align: center;
            white-space: nowrap;
            vertical-align: baseline;
            margin-right: 6px;
        }

        .ptypec {
            display: inline;
            padding: .0em .2em .05em;
            font-size: 85%;
            font-weight: bold;
            line-height: 1;
            background-color: #428bca;
            color: #FFFFFF;
            text-align: center;
            white-space: nowrap;
            vertical-align: baseline;
            margin-right: 6px;
        }

        .ptypep {
            display: inline;
            padding: .0em .2em .05em;
            font-size: 85%;
            font-weight: bold;
            line-height: 1;
            background-color: #6B6B6B;
            color: #FFFFFF;
            text-align: center;
            white-space: nowrap;
            vertical-align: baseline;
            margin-right: 6px;
        }

        /* navigation */
        #nav {
            /* font-family: 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans',
       Corbel, Arial, Helvetica, sans-serif; */
            font-family: Georgia, Helvetica, sans-serif;
            position: fixed;
            top: 50px;
            /* left: 900px; */
            margin-left: 900px;
            /*1060*/
            width: 92px;
            font-size: 15px;
        }

        #nav li2 {
            margin-bottom: 1px;
        }

        ol {
            list-style: none;
        }

        #nav a {
            display: block;
            padding: 6px 9px 7px;
            color: #fff;
            background-color: #455A64;
            text-decoration: none;
        }

        #nav a:hover {
            color: #ffde00;
            /* background-color: #242d36 ; */
        }
    </style>

    <!-- <script type="text/javascript" async="" src="./files/ga.js"></script>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-7953909-1']);
  _gaq.push(['_trackPageview']);

  (function () {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script> -->

    <script type="text/javascript" src="./files/hidebib.js"></script>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
        type="text/css" />
    <!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');



</script>
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-66888300-1', 'auto');
    ga('send', 'pageview');

</script>

<!-- <script src="./files/main.js"></script> -->

<body>
    <ol id="nav">
        <li><a href="#home" title="Home">Home</a></li>
        <!-- <li><a href="#news" title="News">News</a></li> -->
        <li><a href="#pub" title="Papers">Papers</a></li>
        <li><a href="mailto:osilly0616@gmail.com" title="Contact">Contact</a></li>
    </ol>
    <a name="home"></a>
    <div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 150px;">
        <div style="margin: 0px auto; width: 100%;">
            <img title="Wenxuan Huang (ÈªÑÊñáËΩ©)" style="float: left; padding-left: .01em; height: 150px;"
                src="wenxuan.png" />
            <div style="padding-left: 12em; vertical-align: top; height: 130px;"><span
                    style="line-height: 100%; font-size: 20pt;">Wenxuan Huang (ÈªÑÊñáËΩ©)</span>


                <font style="line-height:220%;">&nbsp</font>
                <p>
                    <!-- <a href="https://www.zhihu.com/people/wu-wen-hao-80-23"><img
                            src="https://img.shields.io/badge/Áü•‰πé-0079FF.svg?style=flat-square&logo=zhihu&logoColor=white"
                            height="22px" alt="Áü•‰πé"></a> -->
                    <a href="https://github.com/Osilly"><img
                            src="https://img.shields.io/badge/github-%23121011.svg?style=flat-square&logo=github&logoColor=white"
                            height="22px" alt="github"></a>
                    <a href="https://scholar.google.com/citations?user=6Ys6HgsAAAAJ&hl=en"><img
                            src="https://img.shields.io/badge/Google%20Scholar-4285F4?style=flat-square&logo=google-scholar&logoColor=white"
                            height="22px" alt="Google Scholar"></a>
                    <!-- <a href="https://www.linkedin.com/in/wenhao-w-usyd/"><img
                            src="https://img.shields.io/badge/linkedin-006CAC.svg?&style=flat-square&logo=linkedin&logoColor=white"
                            height="22px" alt="LinkedIn"></a> -->
                    <!-- <a href="https://twitter.com/DrWenhaoWu"><img
                            src="https://img.shields.io/badge/X-%23000000.svg?style=flat-square&logo=X&logoColor=white"
                            height="22px" alt="X"></a> -->
                    <a href="https://www.semanticscholar.org/author/Wenxuan-Huang/2294440519"><img
                            src="https://img.shields.io/static/v1?style=flat-square&message=Semantic+Scholar&color=1857B6&logo=Semantic+Scholar&logoColor=FFFFFF&label="
                            height="22px" alt="Semantic Scholar"></a>
                    <a href="https://orcid.org/0009-0001-9656-813X"><img
                            src="https://img.shields.io/static/v1?style=flat-square&message=ORCID&color=222222&logo=ORCID&logoColor=A6CE39&label="
                            height="22px" alt="ORCID"></a>
                </p>
                <font style="line-height:30%;">&nbsp</font>

                <!-- <p><strong> Ph.D.</strong></p> -->
                <!-- <p><a href="https://sydney.edu.au/engineering/about/school-of-computer-science.html">School of Computer
                        Science</a>, <a href="https://www.sydney.edu.au/engineering/">Faculty of Engineering</a></p> -->
                <p><strong>Master</strong> in <a href="https://english.ecnu.edu.cn/">East China Normal
                        University</a>
                    <!-- <p><strong>RA</strong> in <a href="https://mmlab.ie.cuhk.edu.hk/">MMLab</a>@<a
                        href="https://www.cuhk.edu.hk/">The Chinese University of Hong
                        Kong</a> -->
                    <br /><br />
                    <!-- <p><strong>Senior R&D Engineer @ Baidu Inc.</strong><br /><br /> -->

                    <span><strong>Personal Email </strong>: osilly0616 (at) gmail.com</span> <br />
                    <!-- <span><strong>School Email </strong>: wewu2985 (at) uni.sydney.edu.au</span> <br /> -->
            </div>
        </div>
    </div>


    <div style="clear: both;">
        <div class="section">
            <h2>Short Biography <a href="./files/CV_wenxuan.pdf">[CV]</a> </h2>
            <div class="bio">

                I am the third-year master student in <a href="https://english.ecnu.edu.cn/">East China Normal
                    University</a> under the
                supervision of <a href='https://scholar.google.com/citations?user=k8AMa1kAAAAJ&hl=en'>Prof. Shaohui
                    Lin</a>, while I am also a Research
                Assistant of <a href="https://mmlab.ie.cuhk.edu.hk/">MMLab</a>@<a href="https://www.cuhk.edu.hk/">The
                    Chinese University of Hong Kong</a> and working with <a
                    href='https://scholar.google.com/citations?user=pw_0Z_UAAAAJ&hl=en'>Prof. Wanli
                    Ouyang</a>.
                Furthermore, I focus on AI Research and work closely with industrial AI Laboratory, like <a
                    href="https://www.xiaohongshu.com/">NLP
                    Team@Xiaohongshu</a>.
                <!-- Wenhao earned the Ph.D. degree from the <a href='https://sigmalab-usyd.github.io/'>MMLab</a>, <a
                        href="https://www.sydney.edu.au/">The University of Sydney</a> in 2025, under the
                    supervision of <a href='https://wlouyang.github.io/'>Prof. Wanli Ouyang</a>.
                    Prior to this, he was a full-time senior researcher (3 Years) at <a
                        href="http://vis.baidu.com/">Baidu
                        VIS</a>,
                    where he worked closely with Chief Scientist <a href='https://jingdongwang2017.github.io/'>Dr.
                        Jingdong
                        Wang (IEEE Fellow)</a>. -->

                <!-- I accumulate five years of experience as research intern and full-time senior researcher at the Department of Computer Vision Technology (VIS), Baidu Inc., 
where I worked closely with <a href='https://jingdongwang2017.github.io/'>Dr. Jingdong Wang (IEEE Fellow)</a>.<br/> -->

                <!-- Previously, he received M.S.E degree from
                    <a href='http://english.ucas.ac.cn/'>University of Chinese Academy of Sciences (UCAS)</a>,
                    supervised by
                    <a href='https://scholar.google.com/citations?user=6X77S3cAAAAJ&hl=en'>Prof. Shifeng Chen</a>
                    and <a href='http://mmlab.siat.ac.cn/yuqiao/'>Prof. Yu Qiao</a>.<br />


                    He has spent 8 years (2016-) conducting AI Research & Development in both industry and academic
                    institutions.
                    He is fortunate to have over 6 years of industrial experience at <a
                        href="https://aws.amazon.com/ai/">Amazon AI</a>,
                    <a href="http://research.baidu.com">Baidu AIG</a>, <a href="https://research.snap.com/">Snap
                        Research</a>,
                    <a href="https://www.sensetime.com/en">SenseTime Research</a>, <a
                        href="https://research.samsung.com/">Samsung Research</a>, <a
                        href="https://ir.iqiyi.com/">iQIYI</a>.
                    Additionally, he is/was as a member of academic institutions such as <a
                        href="https://sigmalab-usyd.github.io/">MMLab@USYD</a>,
                    <a href="https://mmlab.ie.cuhk.edu.hk/">MMLab@CUHK</a>, and <a
                        href="https://mmlab.siat.ac.cn/">MMLab@SIAT-CAS</a>.
                    <br>
                    He is honored to be awarded the <a
                        href="http://scholarship.baidu.com/home/index/index#banner"><b>Baidu
                            PhD Fellowship</b></a> (2023).

                    <br /><br /> -->

                <!-- <b>Looking for self-motivated interns in <a href="http://vis.baidu.com/">Baidu VIS</a>. Please drop your CV to me if you are interested in those topics.</b> -->
                <p><i style="color: black; display: inline;">If interested in collaboration or discussion, please
                        email
                        me.</i></p>

                <!-- <i><alert>I am entering the job market starting now (graduate in Spring 2025), and am actively looking for post-doctoral scholar and full-time research scientist position in US. 
  Feel free to schedule a casual chat if our research match :)</alert></i> -->


            </div>
        </div>

        <div style="clear: both;">
            <div class="section">
                <h2>Research Interest</h2>
                <div class="bio">

                    My research interests broadly lie in the areas of <b>Multimodal Large Language Models</b>,
                    especially <b>Multimodal Reasoning Models</b>.
                    current and previous focal areas include:


                    <ul>
                        <li>
                            <strong style="color:#8aa371">Multimodal Large Language Models</strong>
                            <ul>
                                <li><strong style="color:#b58654">Multimodal Reasoning Models (2025-Present)</strong>:
                                    <ul>
                                        <li> <b>Text-based Reasoning</b>:
                                            <a href="https://arxiv.org/abs/2503.06749">Vision-R1 (ICLR 2026)</a><a
                                                href="https://github.com/Osilly/Vision-R1"><img
                                                    src="https://img.shields.io/github/stars/Osilly/Vision-R1?color=success&logo=github"></a>,
                                            <a href="https://arxiv.org/abs/2511.01618">Actial (NeurIPS 2025)</a>,
                                            <a href="https://arxiv.org/abs/2510.06036">Cliff-as-a-judge</a><a
                                                href="https://github.com/MikaStars39/RefusalCliff"><img
                                                    src="https://img.shields.io/github/stars/MikaStars39/RefusalCliff?color=success&logo=github"></a>,
                                            <a href="https://arxiv.org/abs/2505.19714">MT3</a>
                                        </li>
                                    </ul>
                                    <ul>
                                        <li> <b>Agentic Reasoning</b>:
                                            <a>Interleaving Reasoning Survey</a><a
                                                href="https://github.com/Osilly/Awesome-Interleaving-Reasoning"><img
                                                    src="https://img.shields.io/github/stars/Osilly/Awesome-Interleaving-Reasoning?color=success&logo=github"></a>,
                                            <a href="https://arxiv.org/abs/2510.01304">AGILE (ICLR 2026)</a><a
                                                href="https://github.com/yuzeng0-0/AGILE"><img
                                                    src="https://img.shields.io/github/stars/yuzeng0-0/AGILE?color=success&logo=github"></a>,
                                            <a href="https://arxiv.org/abs/2507.20766">RRVF</a><a
                                                href="https://github.com/L-O-I/RRVF"><img
                                                    src="https://img.shields.io/github/stars/L-O-I/RRVF?color=success&logo=github"></a>,
                                            <a href="https://arxiv.org/abs/2506.15451">AgentGroupChat-V2</a><a
                                                href="https://github.com/MikeGu721/AgentGroupChat-V2"><img
                                                    src="https://img.shields.io/github/stars/MikeGu721/AgentGroupChat-V2?color=success&logo=github"></a>

                                        </li>
                                    </ul>
                                </li>
                                <li><strong style="color:#b58654">Multimodal Learning (2024-Present)</strong>:
                                    <a href="https://arxiv.org/abs/2504.17365">TimeSoccer (ACMMM 2025)</a>,
                                    <a href="https://arxiv.org/abs/2503.07487">LLaVA-RadZ</a>
                                </li>
                                <li><strong style="color:#b58654">Benchmarking (2025-Present)</strong>:
                                    <a href="https://arxiv.org/abs/2504.07956">VCR-Bench</a><a
                                        href="https://github.com/zhishuifeiqian/VCR-Bench"><img
                                            src="https://img.shields.io/github/stars/zhishuifeiqian/VCR-Bench?color=success&logo=github"></a>,
                                    <a href="https://arxiv.org/abs/2509.24709">IWR-Bench (ICLR 2026)</a>
                                    <a href="https://github.com/SIGMME/IWR-Bench"><img
                                            src="https://img.shields.io/github/stars/SIGMME/IWR-Bench?color=success&logo=github"></a>
                                </li>
                            </ul>
                        </li>

                        <li>
                            <strong style="color:#8aa371">Efficient Models</strong>
                            <ul>
                                <li><strong style="color:#b58654">Inference Efficiency (2023-Present)</strong>:
                                    <a href="https://arxiv.org/abs/2412.00876">Dynamic-LLaVA (ICLR 2025)</a><a
                                        href="https://github.com/Osilly/dynamic_llava"><img
                                            src="https://img.shields.io/github/stars/Osilly/dynamic_llava?color=success&logo=github"></a>,
                                    <a href="https://arxiv.org/abs/2307.00198">KDFS</a><a
                                        href="https://github.com/Osilly/KDFS"><img
                                            src="https://img.shields.io/github/stars/Osilly/KDFS?color=success&logo=github"></a>
                                </li>
                                <li><strong style="color:#9d7ec8">Training Efficiency (2023-2024)</strong>:
                                    <a href="https://arxiv.org/abs/2404.00672">TokenExpansion (CVPR 2024)</a><a
                                        href="https://github.com/Osilly/TokenExpansion"><img
                                            src="https://img.shields.io/github/stars/Osilly/TokenExpansion?color=success&logo=github"></a>
                                </li>
                            </ul>
                        </li>

                        <li>
                            <strong style="color:#8aa371">AIGC</strong>
                            <ul>
                                <li><strong style="color:#b58654">Unified Model (2025-Present)</strong>:
                                    <a href="https://arxiv.org/abs/2509.06945">IRG (ICLR 2026)</a><a
                                        href="https://github.com/Osilly/Interleaving-Reasoning-Generation"><img
                                            src="https://img.shields.io/github/stars/Osilly/Interleaving-Reasoning-Generation?color=success&logo=github"></a>
                                </li>
                                <li><strong style="color:#b58654">Benchmarking (2025-Present)</strong>:
                                    <a href="https://arxiv.org/abs/2505.12200">CompBench</a><a
                                        href="https://github.com/BhJia/CompBench"><img
                                            src="https://img.shields.io/github/stars/BhJia/CompBench?color=success&logo=github"></a>
                                </li>
                            </ul>
                        </li>
                    </ul>


                    <!-- I've dedicated myself to advanced AI research across these fields, leading to publications in
                    top-tier conferences.
                    As my tech expertise deepened, I now focus less on paper quantity and more on rethinking problems
                    and offering simple, effective solutions. -->


                </div>
            </div>





            <!-- <a name="news"></a>
            <div style="clear: both;">
                <div class="section">
                    <h2>Updates</h2>
                    <div class="paper">
                        <ul>
                            <li><a class="button" href="#" style="color:#FFA500"><strong
                                        style="font-size:16px">New</strong></a> <strong
                                    style="padding-left:5px;">04/2025: </strong>
                                <em> I obtain the <a
                                        href="https://www.daad.de/en/the-daad/postdocnet/fellows/fellows/">DAAD AInet
                                        Fellowship</a>.</em>
                            </li>


                            <li><a class="button" href="#" style="color:#FFA500"><strong
                                        style="font-size:16px">New</strong></a> <strong
                                    style="padding-left:5px;">02/2025: </strong>
                                <em> Our paper <a href="https://arxiv.org/pdf/2411.18180">DistinctAD</a> has been
                                    accepted by <strong><a href="https://cvpr.thecvf.com/Conferences/2025">
                                            <font color="DarkRed">CVPR 2025</font>
                                        </a></strong>
                                    <alert>(Highlight)</alert>. Congratulations to Bo.
                                </em>
                            </li>

                            <li><a class="button" href="#" style="color:#FFA500"><strong
                                        style="font-size:16px">New</strong></a> <strong
                                    style="padding-left:5px;">02/2025: </strong>
                                <em> PhD thesis defense passed! Now looking forward to the conferal in June.
                                </em>
                            </li>

                            <li><a class="button" href="#" style="color:#FFA500"><strong
                                        style="font-size:16px">New</strong></a> <strong
                                    style="padding-left:5px;">09/2024: </strong>
                                <em> [2/2] <a href="https://arxiv.org/pdf/2405.13800">Dense Connecter<a
                                            href="https://github.com/HJYao00/DenseConnector"><img
                                                src="https://img.shields.io/github/stars/HJYao00/DenseConnector?color=success&logo=github"></a>
                                        and
                                        <a href="https://arxiv.org/pdf/2405.11165">AMP<a
                                                href="https://github.com/takomc/amp"><img
                                                    src="https://img.shields.io/github/stars/takomc/amp?color=success&logo=github"></a>
                                            are accepted by
                                            <strong><a href="https://nips.cc/Conferences/2024">
                                                    <font color="DarkRed">NeurIPS 2024</font>
                                                </a></strong>!
                                            <strong>Dense Connector</strong> was cited by <a
                                                href="https://arxiv.org/abs/2409.20566">Apple MM1.5</a>. Congratulations
                                            to Huanjin and Mengxi!
                                </em>
                            </li>
                            <li><a class="button" href="#" style="color:#FFA500"><strong
                                        style="font-size:16px">New</strong></a> <strong
                                    style="padding-left:5px;">07/2024: </strong>
                                <em> Received <a
                                        href="https://en.wikipedia.org/wiki/Chinese_Government_Award_for_Outstanding_Self-Financed_Students_Abroad">
                                        <alert>CGAOSSA Award</alert>
                                    </a> (The highest award granted by the Chinese government to Chinese students
                                    overseas, 650 recipients per year).
                                </em>
                            </li>
                            <li><strong>05/2024: </strong><em>
                                    We explore visual signals, RLHF, and zero-shot image-to-video extension for MLLMs:
                                    (1) We introduce the <a href="https://arxiv.org/pdf/2405.13800">Dense Connecter<a
                                            href="https://github.com/HJYao00/DenseConnector"><img
                                                src="https://img.shields.io/github/stars/HJYao00/DenseConnector?color=success&logo=github"></a>,
                                        a simple, effective, plug-and-play vision-language connector that enhances
                                        existing MLLMs by leveraging multi-layer visual features with minimal
                                        computational overhead.
                                        (2) We present an <a href="https://arxiv.org/pdf/2405.11165">Automated
                                            Multi-level Preference (AMP)<a href="https://github.com/takomc/amp"><img
                                                    src="https://img.shields.io/github/stars/takomc/amp?color=success&logo=github"></a>
                                            framework for RLHF, replacing binary preference learning. It generates
                                            high-quality multi-level preference datasets without human/AI annotators and
                                            employs the multi-level DPO (MDPO) algorithm.
                                            (3) I release <a href="https://arxiv.org/abs/2405.07798">FreeVA</a> <a
                                                href="https://github.com/whwu95/FreeVA"><img
                                                    src="https://img.shields.io/github/stars/whwu95/FreeVA?color=success&logo=github"></a>,
                                            which provides a plug-and-play, simple yet effective study exploring the
                                            utilization of existing image MLLMs as video conversational models in a
                                            <strong>training-free</strong> manner. ‚ö°The core code can be just one
                                            line!</em>
                            <li><strong>05/2024: </strong><em> The <a
                                        href="https://ieeexplore.ieee.org/abstract/document/10670217">extension</a> of
                                    <a href="https://github.com/whwu95/Cap4Video">Cap4Video</a> <a
                                        href="https://github.com/whwu95/Cap4Video"><img
                                            src="https://img.shields.io/github/stars/whwu95/Cap4Video?color=success&logo=github"></a>
                                    has been accepted by <alert>TPAMI</alert>.</em>
                            </li>
                            <li><strong>01/2024: </strong>
                                <em> üéñ I'm honored to be among the <strong>10 PhD students globally</strong> awarded
                                    the 11th <a href="http://scholarship.baidu.com/home/index/index#banner">
                                        <font color="Red"><b>Baidu Scholarship</b></font>
                                    </a>,
                                    a prestigious fellowship in Artificial Intelligence, providing <strong>200,000 RMB
                                        (about $30,000)</strong> to selectees from thousands of applicants. <a
                                        href="https://mp.weixin.qq.com/s/kR16sV_LFn2aO6ZrupHepA">[Á¨¨11Â±äÁôæÂ∫¶Â•ñÂ≠¶ÈáëÊè≠Êôì,
                                        ÂÖ®ÁêÉ10‰∫∫ÂÖ®Âëò95Âêé]</a>
                                </em>
                            </li>
                            <li><strong>11/2023: </strong><em> We release <a
                                        href="https://arxiv.org/abs/2311.15732">GPT4Vis</a> <a
                                        href="https://github.com/whwu95/GPT4Vis"><img
                                            src="https://img.shields.io/github/stars/whwu95/GPT4Vis?color=success&logo=github"></a>,
                                    which provides a <strong>Quantitative Evaluation</strong> of GPT-4 (üò≠Running once
                                    roughly costs üí∞$4000üí∞) for Visual Understanding across images, videos and point
                                    clouds, spinning on 16 datasets.</em>
                            </li>
                            <li><strong>11/2023: </strong><em> We release <a
                                        href="https://arxiv.org/abs/2311.15769">Side4Video</a> <a
                                        href="https://github.com/HJYao00/Side4Video"><img
                                            src="https://img.shields.io/github/stars/HJYao00/Side4Video?color=success&logo=github"></a>,
                                    a Spatial-Temporal Side Network for <strong>Memory-Efficient</strong> Image-to-Video
                                    Transfer Learning, which significantly reduces the training memory cost for action
                                    recognition (‚Üì75%) and text-video retrieval (‚Üì30%).</em>
                            </li>

                            <li><strong>08/2023: </strong><em> The extension of <a
                                        href="https://github.com/whwu95/Text4Vis">Text4Vis</a> <a
                                        href="https://github.com/whwu95/Text4Vis"><img
                                            src="https://img.shields.io/github/stars/whwu95/Text4Vis?color=success&logo=github"></a>
                                    has been accepted by <strong>IJCV</strong>.</em>
                            </li>
                            <li><strong>07/2023: </strong><em> <strong>Two</strong> First-author papers (Temporal
                                    Modeling: <a href="https://github.com/whwu95/ATM">ATM</a> <a
                                        href="https://github.com/whwu95/ATM"><img
                                            src="https://img.shields.io/github/stars/whwu95/ATM?color=success&logo=github"></a>,
                                    Cross-Modal Retrieval: <a href="https://arxiv.org/abs/2301.06309">UA</a> <a
                                        href="https://github.com/bofang98/UATVR"><img
                                            src="https://img.shields.io/github/stars/bofang98/UATVR?color=success&logo=github"></a>)
                                    are accepted by <strong><a href="https://iccv2023.thecvf.com/">
                                            <font color="DarkRed">ICCV2023</font>
                                        </a></strong>.</em>
                            </li>



                            <li><strong>02/2023: </strong><em> <strong>Two</strong> First-author papers for video
                                    understanding (<a href="https://github.com/whwu95/BIKE">BIKE</a> <a
                                        href="https://github.com/whwu95/BIKE"><img
                                            src="https://img.shields.io/github/stars/whwu95/BIKE?color=success&logo=github"></a>,
                                    <a href="https://github.com/whwu95/Cap4Video">Cap4Video</a> <a
                                        href="https://github.com/whwu95/Cap4Video"><img
                                            src="https://img.shields.io/github/stars/whwu95/Cap4Video?color=success&logo=github"></a>)
                                    are accepted by <strong><a href="https://cvpr2023.thecvf.com/">
                                            <font color="DarkRed">CVPR 2023</font>
                                        </a></strong>.
                                    <a href="https://github.com/whwu95/Cap4Video">Cap4Video</a> involves GPT to enhance
                                    text-video learning,
                                    is selected as a <font color="Red"><b>Highlight</b></font> paper (<strong>Top
                                        2.5%</strong>).</em>
                            </li>

                            <li><strong>11/2022: </strong><em> <strong>Two</strong> papers (Video Recognition: <a
                                        href="https://arxiv.org/pdf/2207.01297.pdf">Text4Vis</a> <a
                                        href="https://github.com/whwu95/Text4Vis"><img
                                            src="https://img.shields.io/github/stars/whwu95/Text4Vis?color=success&logo=github"></a>,
                                    Style Transfer: <a href="https://arxiv.org/pdf/2212.01567.pdf">AdaCM</a>) are
                                    accepted by
                                    <strong><a href="https://aaai.org/Conferences/AAAI-23/">
                                            <font color="DarkRed">AAAI 2023</font>
                                        </a></strong>.</em>
                            </li>

                            <li> <strong>07/2022: </strong><em> <strong>Three</strong> papers (Video Sampling: <a
                                        href="https://arxiv.org/pdf/2207.10388.pdf">NSNet</a>, <a
                                        href="https://arxiv.org/pdf/2207.10379.pdf">TSQNet</a>, Cross-Modal Learning: <a
                                        href="https://arxiv.org/pdf/2208.09843.pdf">CODER</a>) are accepted by
                                    <strong><a href="https://eccv2022.ecva.net/">
                                            <font color="DarkRed">ECCV 2022</font>
                                        </a></strong>.</em>
                            </li>

                            <li> <strong>06/2022: </strong>
                                <em> Our <a href="https://dl.acm.org/doi/10.1145/3503161.3547888">MaMiCo</a>,
                                    a new video self-supervised learning work, is accepted by
                                    <strong><a href="https://2022.acmmm.org/">
                                            <font color="DarkRed">ACMMM 2022</font>
                                        </a></strong>(<font color="Red"><b>Oral Presentation</b></font>).</em>
                            </li>

                            <li> <strong>03/2022: </strong>
                                <em> <strong>Two</strong> low-level vision papers (<a
                                        href="https://arxiv.org/pdf/2203.12707.pdf">MSPC</a>, <a
                                        href="https://arxiv.org/pdf/2203.00911.pdf">BAIRNet</a>) are accepted by
                                    <strong><a href="https://cvpr2022.thecvf.com/">
                                            <font color="DarkRed">CVPR 2022</font>
                                        </a></strong>.</em>
                            </li>

                            <li> <strong>12/2021: </strong>
                                <em> Our <a href="https://arxiv.org/pdf/2112.07984.pdf">BCNet</a> <a
                                        href="https://github.com/happy-hsy/BCNet"><img
                                            src="https://img.shields.io/github/stars/happy-hsy/BCNet?color=success&logo=github"></a>,
                                    an general temporal localization framework, is accepted by
                                    <strong><a href="https://aaai.org/Conferences/AAAI-22/">
                                            <font color="DarkRed">AAAI 2022</font>
                                        </a></strong>.</em>
                            </li>

                            <li> <strong>07/2021: </strong>
                                <em> Our <a href="https://arxiv.org/pdf/2106.02342.pdf">ASCNet</a>,
                                    a self-supervised video representation learning framework, is accepted by
                                    <strong><a href="https://iccv2021.thecvf.com/">
                                            <font color="DarkRed">ICCV 2021</font>
                                        </a></strong>.</em>
                            </li>

                            <li> <strong>07/2021: </strong>
                                <em> <strong>Two</strong> papers (Video Recognition, Crowd Counting) are accepted by
                                    <strong><a href="https://2021.acmmm.org/">
                                            <font color="DarkRed">ACMMM 2021</font>
                                        </a></strong>.</em>
                            </li>

                            <li> <strong>04/2021: </strong>
                                <em> We present a novel task:
                                    <a href="https://arxiv.org/pdf/2108.03825.pdf">Weakly-Supervised Spatio-Temporal
                                        Anomaly Detection</a>,
                                    is accepted by
                                    <strong><a href="https://ijcai-21.org/">
                                            <font color="DarkRed">IJCAI 2021</font>
                                        </a></strong>.</em>
                            </li>


                            <li> <strong>04/2021: </strong>
                                <em>
                                    <font color="Red"><b>Winner</b></font> in the Traffic Anomaly Detection Track of the
                                    <strong><a href="https://www.aicitychallenge.org/">
                                            <font color="DarkRed">CVPR 2021 AI CITY CHALLENGE</font>
                                        </a></strong>.
                                </em>
                            </li>


                            <li> <strong>12/2020: </strong>
                                <em>Our <a href="https://arxiv.org/pdf/2012.06977.pdf">MVFNet</a> <a
                                        href="https://github.com/whwu95/MVFNet"><img
                                            src="https://img.shields.io/github/stars/whwu95/MVFNet?color=success&logo=github"></a>,
                                    an efficient temporal module, is accepted by
                                    <strong><a href="https://aaai.org/Conferences/AAAI-21/">
                                            <font color="DarkRed">AAAI 2021</font>
                                        </a></strong>.</em>
                            </li>



                            <li> <strong>07/2020: </strong>
                                <em>Our <a href="https://arxiv.org/pdf/2012.02994.pdf">ADD-GCN</a> <a
                                        href="https://github.com/Yejin0111/ADD-GCN"><img
                                            src="https://img.shields.io/github/stars/Yejin0111/ADD-GCN?color=success&logo=github"></a>
                                    for multi-label image recognition,
                                    is accepted by
                                    <strong><a href="https://eccv2020.ecva.net/">
                                            <font color="DarkRed">ECCV 2020</font>
                                        </a></strong>.</em>
                            </li>

                            <li> <strong>05/2020: </strong>
                                <em>One <a
                                        href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w40/Wu_Dynamic_Inference_A_New_Approach_Toward_Efficient_Video_Action_Recognition_CVPRW_2020_paper.pdf">dynamic
                                        video inference</a> paper
                                    is accepted for <b>Oral Presentation</b> on <font color="Green">CVPR2020 EDLCV
                                        workshop</font>.</em>
                            </li>

                            <li> <strong>07/2019: </strong>
                                <em>My <alert>first</alert> paper <a
                                        href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Multi-Agent_Reinforcement_Learning_Based_Frame_Sampling_for_Effective_Untrimmed_Video_ICCV_2019_paper.pdf">MARL</a>,
                                    a novel video sampler, is accepted as <font color="Red"><b>Oral Presentation</b>
                                    </font> (<strong>Top 4%</strong>) on
                                    <strong><a href="https://iccv2019.thecvf.com/">
                                            <font color="DarkRed">ICCV 2019</font>
                                        </a></strong>.</em>
                            </li>

                            <li> <strong>09/2017: </strong>
                                <em>Recommended to <a href='http://english.ucas.ac.cn/'>University of Chinese Academy of
                                        Sciences</a> towards Master degree <alert>with exam exemption (‰øùÈÄÅÁ†îÁ©∂Áîü)</alert>
                                    .</em>
                            </li>
                            <li> <strong>06/2017: </strong>
                                <em>Graduated from Central South University with <alert> Outstanding Graduate Honor
                                    </alert>.</em>
                            </li>
                            <li> <strong>10/2016: </strong>
                                <em>Joined <a href='http://mmlab.siat.ac.cn'>MMLab@SIAT</a> as research intern. Started
                                    working on Computer
                                    Vision.</em>
                            </li>

                        </ul>
                    </div>
                </div>
            </div> -->









            <a name="pub"></a>
            <!-- <h2 id="confpapers">Selected Publications [ <a href="./publication.html">Full List</a> -->
            <h2 id="confpapers">Selected Publications [ <a
                    href="https://scholar.google.com/citations?user=6Ys6HgsAAAAJ&hl=en">Full List</a>
                ] </h2>
            <b>( *Co-first Author, <sup><span lang="EN-US"
                        style="mso-bidi-font-size:8pt;font-family:Wingdings;mso-ascii-font-family:'Times New Roman';mso-hansi-font-family:'Times New Roman';mso-char-type:symbol;mso-symbol-font-family:Wingdings">*</span></sup>Correspondence,
                <sup title="Project Leader" style="font-size:8pt; margin-left:4px;">‚úù</sup> Project Leader)</b>

            <div class="section">
                <div class="bio">

                    <div class="paper" id="xxx"><img class="paper" src="./pub/vision_r1.png" />
                        <div>
                            <b>[Reasoning MLLM]</b> <a><b>Vision-R1: Incentivizing Reasoning Capability in Multimodal
                                    Large Language Models</b></a><br />
                            <b style="color:darkred"><u>Wenxuan Huang</u></b>, Bohan Jia, Zijie
                            Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, Shaohui Lin<br />
                            <i><a style="color:green"><b>Citation 300+ & Star 700+ in six months</b></a>,
                                <i>International Conference on Learning Representations <b>
                                        <font color="DarkRed">(ICLR)</font>
                                    </b>, 2026, first author</i><br />
                                [ <a href='https://arxiv.org/abs/2503.06749'>Paper</a> ]
                                [ <a href='https://github.com/Osilly/Vision-R1'>Code</a> ]
                                <!-- [ <a href='https://zhuanlan.zhihu.com/p/700000183'>My Blog (Chinese)</a> ] -->
                                <br />
                                <strong>
                                    This is the first paper to explore how to effectively use R1-like RL for MLLMs and
                                    introduce Vision-R1, a reasoning MLLM that leverages cold-start initialization and
                                    RL
                                    training to incentivize reasoning capability.
                                </strong>
                                <!-- <alert>
                                This is the first paper to explore how to effectively use R1-like RL for MLLMs and
                                introduce Vision-R1, a reasoning MLLM that leverages cold-start initialization and RL
                                training to incentivize reasoning capability.
                            </alert> -->
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/IRG.png" />
                        <div>
                            <b>[AIGC/Interleaving Reasoning/Unified MLLM]</b> <a><b>Interleaving Reasoning for Better
                                    Text-to-image Generation</b></a><br />
                            <b style="color:darkred"><u>Wenxuan Huang</u></b>, Shuang Chen, Zheyong Xie, Shaosheng
                            Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo
                            Qiao, Hangyu Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin
                            <br />
                            <i>International Conference on Learning Representations <b>
                                    <font color="DarkRed">(ICLR)</font>
                                </b>, 2026, first author</i><br />
                            [ <a href='https://arxiv.org/abs/2509.06945'>Paper</a> ]
                            [ <a href='https://github.com/Osilly/Interleaving-Reasoning-Generation'>Code</a> ]
                            <br />
                            <strong>
                                This is an early exploration to introduce Interleaving Reasoning to Text-to-image
                                Generation field and achieve the SoTA benchmark performance. It also significantly
                                improves the quality, fine-grained details and aesthetic aspects of generated images.
                            </strong>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/dynamic_llava.png" />
                        <div>
                            <b>[Efficient MLLM]</b> <a><b>Dynamic-LLaVA: Efficient Multimodal
                                    Large Language Models via Dynamic Vision-language
                                    Context Sparsification</b></a><br />
                            <b style="color:darkred"><u>Wenxuan Huang</u></b>, Zijie Zhai, Yunhang Shen, Shaosheng Cao,
                            Fei Zhao, Xiangfeng Xu, Zheyu Ye, Shaohui Lin
                            <br />
                            <i>International Conference on Learning Representations <b>
                                    <font color="DarkRed">(ICLR)</font>
                                </b>, 2025, first author</i><br />
                            [ <a href='https://arxiv.org/abs/2412.00876'>Paper</a> ]
                            [ <a href='https://github.com/Osilly/dynamic_llava'>Code</a> ]
                            <br />
                            <strong>
                                Dynamic-LLaVA is the first MLLM acceleration framework that simultaneously sparsifies
                                both vision and language contexts while integrating inference efficiency optimization
                                across different MLLM inference modes into a unified framework.
                            </strong>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/toe.png" />
                        <div>
                            <b>[Transformer Training Acceleration]</b> <a><b>A General and Efficient Training for
                                    Transformer via Token Expansion</b></a><br />
                            <b style="color:darkred"><u>Wenxuan Huang</u></b>, Yunhang Shen, Jiao Xie, Baochang Zhang,
                            Gaoqi He, Ke Li, Xing Sun, Shaohui Lin
                            <br />
                            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>
                                    <font color="DarkRed">(CVPR)</font>
                                </b>, 2024, first author</i><br />
                            [ <a href='https://arxiv.org/abs/2404.00672'>Paper</a> ]
                            [ <a href='https://github.com/Osilly/TokenExpansion'>Code</a> ]
                            <br />
                            <strong>
                                We proposed one plug-and-play Transformer training acceleration framework, without
                                twisting the original training hyper-parameters, architecture, and introducing
                                additional training strategies.
                            </strong>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/G-LA-MSG.png" />
                        <div>
                            <b>[AI4Geophysic]</b> <a><b>An Intelligent First Arrival Picking Method of Microseismic
                                    Signals Based on the Small Sample Expansion</b></a><br />
                            <b style="color:darkred"><u>Wenxuan Huang</u></b>, Guanqun Sheng, Xingong Tang, Kai Ma,
                            Jingyi Lu, Hang Sun
                            <br />
                            <i>IEEE Transactions on Geoscience and Remote Sensing <b>
                                    <font color="DarkRed">(TGRS)</font>
                                </b>, first author</i><br />
                            [ <a href='https://ieeexplore.ieee.org/abstract/document/10972295'>Paper</a> ]
                            [ <a href='https://github.com/Osilly/G-LA-MSG-and-AOG-PSPNet'>Code</a> ]
                            <br />
                            <strong>
                                We proposed one GAN to generation the microseismic samples under unsupervised conditions
                                to expand the microseismic data having a limited number of samples. Then we use the
                                enhanced first arrival picking network to improve the accuracy of first arrivals of low
                                SNR microseismic signals.
                            </strong>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/agile.png" />
                        <div>
                            <b>[Agentic RL/Agency task]</b> <a><b>Agentic Jigsaw Interaction Learning for Enhancing
                                    Visual Perception and Reasoning in Vision-Language Models</b></a><br />
                            Yu Zeng*, <b style="color:darkred"><u>Wenxuan Huang</u>*</b>, Shiting Huang*, Xikun Bao,
                            Yukun Qi, Yiming Zhao, Qiuchen Wang, Lin Chen, Zehui Chen, Huaian Chen, Wanli Ouyang, Feng
                            Zhao
                            <br />
                            <i>International Conference on Learning Representations <b>
                                    <font color="DarkRed">(ICLR)</font>
                                </b>, 2026, co-first author (second)</i><br />
                            [ <a href='https://arxiv.org/abs/2510.01304'>Paper</a> ]
                            [ <a href='https://github.com/yuzeng0-0/AGILE'>Code</a> ]
                            <br />
                            <strong>
                                We introduce agentic jigsaw interaction learning to enhance visual perception and
                                reasoning in MLLMs without VQA labels during training, demonstrating strong
                                generalization across 9 general vision tasks.
                            </strong>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/actial.png" />
                        <div>
                            <b>[Reasoning MLLM]</b> <a><b>Actial: Activate Spatial Reasoning Ability of Multimodal Large
                                    Language Models</b></a><br />
                            Xiaoyu Zhan*, <b style="color:darkred"><u>Wenxuan Huang</u>*<sup>‚úù</sup></b>,
                            Hao Sun*, Xinyu Fu, Changfeng Ma, Shaosheng Cao, Bohan Jia, Shaohui Lin, Zhenfei Yin, Lei
                            Bai, Wanli Ouyang,
                            Yuanqi Li, Jie Guo, Yanwen Guo
                            <br />
                            <i>Conference on Neural Information Processing Systems <b>
                                    <font color="DarkRed">(NeurIPS)</font>
                                </b>, 2025, co-first author (second) & project leader</i><br />
                            <!-- [ <a href='https://ieeexplore.ieee.org/abstract/document/10972295'>Paper</a> ]
                            [ <a href='https://github.com/Osilly/G-LA-MSG-and-AOG-PSPNet'>Code</a> ]
                            <br /> -->
                            <strong>
                                We attempt to solve that current MLLMs cannot effectively capture the detailed spatial
                                information required for robust real-world performance, especially cross-view
                                consistency, a key requirement for accurate 3D reasoning.
                            </strong>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/timesoccer.png" />
                        <div>
                            <b>[MLLM]</b> <a><b>TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer
                                    Commentary Generation</b></a><br />
                            Ling You*, <b style="color:darkred"><u>Wenxuan Huang</u>*</b>, Xinni Xie, Xiangyi Wei,
                            Bangyan Li, Shaohui Lin, Yang Li, Changbo Wang.
                            <br />
                            <i>ACM International Conference on Multimedia <b>
                                    <font color="DarkRed">(ACMMM)</font>
                                </b>, 2025, co-first author (second)</i><br />
                            [ <a href='https://arxiv.org/abs/2504.17365'>Paper</a> ]
                            [ <a href='https://vpx-ecnu.github.io/TimeSoccer-Website/'>Project Page</a> ]
                            <!-- [ <a href='https://github.com/Osilly/G-LA-MSG-and-AOG-PSPNet'>Code</a> ] -->
                            <br />
                            <strong>
                                We propose the first end-to-end MLLM for soccer commentary generation, specifically
                                designed for Single-anchor Dense Video Captioning (SDVC) in full-match soccer videos.
                                The model jointly predicts timestamps and generates captions in a single pass, enabling
                                global context modeling over 45-minute matches.
                            </strong>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/llava_radz.png" />
                        <div>
                            <b>[MLLM]</b> <a><b>LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle
                                    Zero-shot Radiology Recognition?</b></a><br />
                            Bangyan Li*, <b style="color:darkred"><u>Wenxuan Huang</u>*<sup><span
                                        style="font-family:Wingdings">*</span></sup></b>, Zhenkun Gao,
                            Yeqiang Wang, Yunhang Shen, Jingzhong Lin, Ling You, Yuxiang Shen, Shaohui Lin, Wanli
                            Ouyang, Yuling Sun
                            <br />
                            <i>Preprint, co-first author (second) & corresponding author</i><br />
                            [ <a href='https://arxiv.org/abs/2503.07487'>Paper</a> ]
                            <br />
                            <strong>
                                We convert generative models to discriminative models to address the limitation that
                                current MLLMs cannot effectively tackle zero-shot radiology recognition.
                            </strong><br />
                            <i>Label: <alert>Wenxuan Huang</alert> proposed the main idea and designed the experiments,
                                contributing to the discussion of this paper. <alert>Bangyan Li</alert> refined and
                                finalized
                                the idea, implemented the code and experiments, and was responsible for writing the
                                manuscript</i>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/compbench.png" />
                        <div>
                            <b>[Image Editing Benchmark]</b> <a><b>CompBench: Benchmarking Complex Instruction-guided
                                    Image Editing</b></a><br />
                            Bohan Jia*, <b style="color:darkred"><u>Wenxuan Huang</u>*</b>, Yuntian Tang*, Junbo Qiao,
                            Jincheng Liao, Shaosheng Cao, Fei Zhao, Zhaopeng Feng, Zhouhong Gu, Zhenfei Yin, Lei Bai,
                            Wanli Ouyang, Lin Chen, Zihan Wang, Yuan Xie, Shaohui Lin
                            <br />
                            <i>Preprint, co-first author (second)</i><br />
                            [ <a href='https://arxiv.org/abs/2505.12200'>Paper</a> ]
                            [ <a href='https://github.com/BhJia/CompBench'>Code</a> ]
                            <br />
                            <strong>
                                We propose the first benchmark for complex instruction-guided image editing.
                            </strong>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/clip_map.png" />
                        <div>
                            <b>[CLIP Inference Acceleration]</b> <a><b>CLIP-Map: Structured Matrix Adaptation for
                                    Parameter-Efficient CLIP Compression</b></a><br />
                            Kangjie Zhang*, <b style="color:darkred"><u>Wenxuan Huang</u>*</b>, Xin Zhou, Boxiang Zhou,
                            Dejia Song, Yuan Xie, Baochang Zhang, Lizhuang Ma, Nemo Chen, Xu Tang, Yao Hu, Shaohui Lin
                            <br />
                            <i>Preprint, co-first author (second)</i><br />
                            <strong>
                                We propose the first mapping-based CLIP compression framework that maps CLIP parameters
                                to a smaller representation, thereby accelerating inference.
                            </strong><br />
                            <i>Label: <alert>Wenxuan Huang</alert> proposed the main idea and designed the experiments,
                                contributing to the discussion of this paper. <alert>Kangjie Zhang</alert> refined and
                                finalized
                                the idea, implemented the code and experiments, and was responsible for writing the
                                manuscript</i>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/kdfs.png" />
                        <div>
                            <b>[CNN Inference Acceleration]</b> <a><b>Filter Pruning for Efficient CNNs via
                                    Knowledge-driven Differential Filter Sampler</b></a><br />
                            <b style="color:grey">Shaohui Lin</b>, <b style="color:darkred"><u>Wenxuan Huang</u></b>,
                            Jiao Xie, Baochang Zhang,
                            Yunhang Shen, Zhou Yu, Jungong Han, David Doermann
                            <br />
                            <i>Preprint, first student author (first author is my advisor)</i><br />
                            [ <a href='https://arxiv.org/abs/2307.00198'>Paper</a> ]
                            [ <a href='https://github.com/Osilly/KDFS'>Code</a> ]
                            <br />
                            <strong>
                                We propose a unified CNN pruning framework directly optimized end-to-end with a global
                                pruning constraint.
                            </strong>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <div class="paper" id="xxx"><img class="paper" src="./pub/visual_cgec.png" />
                        <div>
                            <b>[Reasoning MLLM]</b> <a><b>Exploring End-to-End Paradigms for Visual Chinese Grammatical
                                    Error Correction</b></a><br />
                            Xiaoman Wang, <u><b style="color:darkred">Wenxuan Huang</b></u>, Wenbiao Tao, Yike Zhao,
                            Yaohui Liu, Yunshi Lan, Weining Qian
                            <br />
                            <i>Preprint, second author</i><br />
                            <strong>
                                We explore how to fine-turn one MLLM to solve the complex perceptron task.
                            </strong>
                        </div>
                        <div class="spanner"></div>
                    </div>

                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/NIPS2024/dc.png" />
                        <div>
                            <a><b>Dense Connector for MLLMs</b></a><br />
                            Huanjin Yao, <u><b style="color:darkred">Wenhao Wu</u>*<sup><span
                                    style="font-family:Wingdings">*</span></sup></b>, Taojiannan Yang, Yuxin Song,
                            Mengxi Zhang, Haocheng Feng,
                            Yifan Sun, Zhiheng Li, Wanli Ouyang, Jingdong Wang<br />
                            <i>Conference on Neural Information Processing Systems <b>
                                    <font color="DarkRed">(NeurIPS)</font>
                                </b>, 2024</i><br />
                            [ <a href='https://arxiv.org/pdf/2405.13800'>PDF</a> ]
                            [ <a href='https://github.com/HJYao00/DenseConnector'>Code</a> ]
                            [ <a href='https://zhuanlan.zhihu.com/p/700000183'>My Blog (Chinese)</a> ]
                            <br /><strong>A universal plug-and-play module to enhance Multimodal-LLM.</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->

                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/NIPS2024/amp.png" />
                        <div>
                            <a><b>Automated Multi-level Preference for MLLMs</b></a><br />
                            Mengxi Zhang, <u><b style="color:darkred">Wenhao Wu</b></u>, Yu Lu, Yuxin Song, Kang Rong,
                            Huanjin Yao, Jianbo Zhang, Fanglong Liu, Yifan Sun, Haocheng Feng, Jingdong Wang<br />
                            <i>Conference on Neural Information Processing Systems <b>
                                    <font color="DarkRed">(NeurIPS)</font>
                                </b>, 2024</i><br />
                            [ <a href='https://arxiv.org/pdf/2405.11165'>PDF</a> ]
                            [ <a href='https://github.com/takomc/amp'>Code</a> ]
                            [ <a href='https://zhuanlan.zhihu.com/p/700308854'>My Blog (Chinese)</a> ]
                            <br /><strong>We present an AMP for RLHF, replacing binary preference learning. It generates
                                high-quality multi-level preference datasets without human/AI annotators.</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->

                    <!-- Arxiv -->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/Arxiv/freeva.png" />
                        <div>
                            <a><b>FreeVA: Offline MLLM as Training-Free Video Assistant</b></a><br />
                            <u><b style="color:darkred">Wenhao Wu</u></b><br />
                            <i>Technical Report, 2024</i><br />
                            [ <a href='https://arxiv.org/pdf/2405.07798'>PDF</a> ]
                            [ <a href='https://github.com/whwu95/FreeVA'>Code</a> ]
                            <br /><strong>FreeVA - a plug-and-play, simple yet effective study exploring the utilization
                                of existing image MLLMs as video conversational models in a training-free
                                manner.</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->

                    <!-- Arxiv -->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/Arxiv/GPT4Vis.png" />
                        <div>
                            <a><b>GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?</b></a><br />
                            <u><b style="color:darkred">Wenhao Wu</b></u>, Huanjin Yao, Mengxi Zhang, Yuxin Song, Wanli
                            Ouyang, Jingdong Wang<br />
                            <i>Technical Report, 2023</i><br />
                            [ <a href='https://arxiv.org/pdf/2311.15732.pdf'>PDF</a> ]
                            [ <a href='https://github.com/whwu95/GPT4Vis'>Code</a> ]
                            [ <a href='https://zhuanlan.zhihu.com/p/669758735'>My Blog (Chinese)</a> ]
                            <br />
                            <alert>We provide a Quantitative Evaluation of GPT-4 for Visual Understanding across images,
                                videos and point clouds, spinning on 16 popular datasets.</alert>
                            <br />(üò≠Running all tests once roughly costs üí∞$4000+üí∞)
                        </div>
                        <div class="spanner"></div>
                    </div> -->




                    <!-- Arxiv -->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/Arxiv/side4video.png" />
                        <div>
                            <a><b>Side4Video: Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer
                                    Learning</b></a><br />
                            Huanjin Yao, <u><b style="color:darkred">Wenhao Wu</u>*<sup><span
                                    style="font-family:Wingdings">*</span></sup></b>, Zhiheng Li<br />
                            <i>Technical Report, 2023</i><br />
                            [ <a href='https://arxiv.org/pdf/2311.15769.pdf'>PDF</a> ]
                            [ <a href='https://github.com/HJYao00/Side4Video'>Code</a> ]
                            <br /><strong>Side4Video significantly reduces the training memory cost for action
                                recognition (‚Üì75%) and text-video retrieval (‚Üì30%).</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->

                    <!-- ICCV23 -->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/ICCV2023/ATM.png" />
                        <div>
                            <a><b>What Can Simple Arithmetic Operations Do for Temporal Modeling?</b></a><br />
                            <u><b style="color:darkred">Wenhao Wu</b></u>, Yuxin Song, Zhun Sun, Jingdong Wang, Chang
                            Xu, Wanli Ouyang<br />
                            <i>IEEE International Conference on Computer Vision <b>
                                    <font color="DarkRed">(ICCV)</font>
                                </b>, 2023</i><br />
                            [ <a href='https://arxiv.org/pdf/2307.08908.pdf'>PDF</a> ]
                            [ <a href='https://github.com/whwu95/ATM'>Code</a> ]
                        </div>
                        <div class="spanner"></div>
                    </div> -->

                    <!-- CVPR23 -->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/CVPR2023/cap4video.png" />
                        <div>
                            <a><b>Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?</b></a><br />
                            <u><b style="color:darkred">Wenhao Wu</b></u>, Haipeng Luo, Bo Fang, Jingdong Wang, Wanli
                            Ouyang<br />
                            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>
                                    <font color="DarkRed">(CVPR)</font>
                                </b>, 2023 </i><br />
                            <b>
                                <font color="Red">[Highlight, Top 2.5% of 9155 submissions]</font>
                            </b>
                            [ <a href='https://arxiv.org/pdf/2301.00184.pdf'>PDF</a> ]
                            [ <a href='https://github.com/whwu95/Cap4Video'>Code</a> ]<br /><br />

                            <a><b>Cap4Video++: Enhancing Video Understanding with Auxiliary Captions</b></a><br />
                            <u><b style="color:darkred">Wenhao Wu</b></u>, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi
                            Yang, Wanli Ouyang<br />
                            <i>Transactions on Pattern Analysis and Machine Intelligence <b>
                                    <font color="DarkRed">(TPAMI)</font>
                                </b>, 2024</i> <br />
                            <alert>Impact factor: 23.6</alert> <br />
                            [ <a href='https://ieeexplore.ieee.org/document/10670217'>PDF</a> ]
                            <br /><strong>Cap4Video leverages auxiliary captions generated by GPT to enhance cross-modal
                                learning.</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->

                    <!-- ICCV23 -->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/ICCV2023/UA.png" />
                        <div>
                            <a><b>UATVR: Uncertainty-Adaptive Text-Video Retrieval</b></a><br />
                            Bo Fang*, <u><b style="color:darkred">Wenhao Wu*</b></u>, Chang Liu, Yu Zhou, Yuxin Song,
                            Weiping Wang, Xiangbo Shu, Xiangyang Ji, Jingdong Wang<br />
                            <i>IEEE International Conference on Computer Vision <b>
                                    <font color="DarkRed">(ICCV)</font>
                                </b>, 2023</i><br />
                            [ <a href='https://arxiv.org/pdf/2301.06309.pdf'>PDF</a> ]
                            [ <a href="https://github.com/bofang98/UATVR">Code</a> ]
                        </div>
                        <div class="spanner"></div>
                    </div> -->



                    <!-- CVPR23-->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/CVPR2023/BIKE.png" />
                        <div>
                            <a><b>Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained
                                    Vision-Language
                                    Models</b></a><br />
                            <u><b style="color:darkred">Wenhao Wu</b></u>, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi
                            Yang, Wanli
                            Ouyang<br />
                            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>
                                    <font color="DarkRed">(CVPR)</font>
                                </b>, 2023 </i><br />
                            [ <a href='https://arxiv.org/pdf/2301.00182.pdf'>PDF</a> ]
                            [ <a href='https://github.com/whwu95/BIKE'>Code</a> ]
                        </div>
                        <div class="spanner"></div>
                    </div> -->


                    <!-- AAAI2023 -->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/AAAI2023/text4vis.png" />
                        <div>
                            <a><b>Revisiting Classifier: Transferring Vision-Language Models for Video
                                    Recognition</b></a><br />
                            <u><b style="color:darkred">Wenhao Wu</b></u>, Zhun Sun, Wanli Ouyang<br />
                            <i>The AAAI Conference on Artificial Intelligence <b>
                                    <font color="DarkRed">(AAAI)</font>
                                </b>, 2023</i> <br />
                            [ <a href='https://arxiv.org/pdf/2207.01297.pdf'>PDF</a> ]
                            [ <a href='https://github.com/whwu95/Text4Vis'>Code</a> ]
                            [ <a href="papers/AAAI2023/text4vis_AAAI23_Poster_Wenhao.pdf">Poster</a> ]
                            [ <a href="papers/AAAI2023/text4vis-aaai2023-presentation.pdf">Slides</a> ]
                            [ <a href=''>Video</a> ] <br /><br />

                            <a><b>Transferring Vision-Language Models for Visual Recognition: A Classifier
                                    Perspective</b></a><br />
                            <u><b style="color:darkred">Wenhao Wu</b></u>, Zhun Sun, Yuxin Song, Jingdong Wang, Wanli
                            Ouyang<br />
                            <i>International Journal of Computer Vision <b>
                                    <font color="DarkRed">(IJCV)</font>
                                </b>, 2023</i>
                            <alert>Impact factor: 19.5</alert> <br />
                            [ <a href='https://link.springer.com/article/10.1007/s11263-023-01876-w'>PDF</a> ]
                            <br />
                            <strong>We revisit the classifier with the textual embeddings, and achieve SOTA performance
                                on
                                Full-supervision/Few-shot/Zero-shot recognition.</strong>

                        </div>
                        <div class="spanner"></div>
                    </div> -->

                    <!-- ECCV2022-->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/ECCV2022/nsnet.png" />
                        <div>
                            <a><b>NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition</b></a><br />
                            Boyang Xia*, <u><b style="color:darkred">Wenhao Wu</u>*<sup><span
                                    style="font-family:Wingdings">*</span></sup></b>,
                            Haoran Wang, Rui Su,
                            Dongliang He, Haosen Yang,
                            Xiaoran Fan, Wanli Ouyang<br />
                            <i>European Conference on Computer Vision<b>
                                    <font color="DarkRed">(ECCV)</font>
                                </b>, 2022</i> <br />
                            [ <a href='https://arxiv.org/pdf/2207.10388.pdf'>PDF</a> ]
                            [ <a href='https://lawrencexia2008.github.io/projects/nsnet'>Project</a> ] <br />
                            <strong>A sampler with a 4x faster practical speed than SOTA methods.</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->


                    <!-- ECCV2022-->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/ECCV2022/tsqnet.png" />
                        <div>
                            <a><b>Temporal Saliency Query Network for Efficient Video Recognition</b></a><br />
                            Boyang Xia*, Zhihao Wang*, <u><b style="color:darkred">Wenhao Wu</u><sup><span
                                    style="font-family:Wingdings">*</span></sup></b>,
                            Haoran
                            Wang, Jungong Han<br />
                            <i>European Conference on Computer Vision<b>
                                    <font color="DarkRed">(ECCV)</font>
                                </b>, 2022</i> <br />

                            [ <a href='https://arxiv.org/pdf/2207.10379.pdf'>PDF</a> ]
                            [ <a href='https://lawrencexia2008.github.io/projects/tsqnet'>Project</a> ] <br />
                            <strong>TSQNet, the first work to model temporal sampling as a query-response task.</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->

                    <!-- ACMMM2022-->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/ACMMM2O22/mamico.png" />
                        <div>
                            <a><b>MaMiCo: Macro-to-Micro Semantic Correspondence for Self-supervised Video
                                    Representation
                                    Learning</b></a><br />
                            Bo Fang*, <u><b style="color:darkred">Wenhao Wu</u>*</b>, Chang Liu*, Yu Zhou, Dongliang He,
                            Weiping
                            Wang<br />
                            <i>ACM International Conference on Multimedia<b>
                                    <font color="DarkRed">(ACMMM)</font>
                                </b>, 2022</i> <br />
                            <b>
                                <font color="Red">[Oral, 5.0% acceptance rate]</font>
                            </b>
                            [ <a href='https://dl.acm.org/doi/10.1145/3503161.3547888'>PDF</a> ]
                            [ <a href=''>Project</a> ] <br />
                            <strong>MaMiCo, a self-supervised Macro-to-Micro Semantic Correspondence learning framework
                                for video
                                representation learning.</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->

                    <!-- AAAI2022-->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/AAAI2022/BCNet.png" />
                        <div>
                            <a><b>Temporal Action Proposal Generation with Background Constraint</b></a><br />
                            Haosen Yang*, <u><b style="color:darkred">Wenhao Wu</u>*</b>, Lining Wang, Sheng Jin, Boyang
                            Xia,
                            Hongxun Yao, Hujie
                            Huang<br />
                            <i>The AAAI Conference on Artificial Intelligence <b>
                                    <font color="DarkRed">(AAAI)</font>
                                </b>, 2022</i> <br />
                            <b>
                                <font color="Green">[15% acceptance rate]</font>
                            </b>
                            [ <a href='https://arxiv.org/pdf/2112.07984.pdf'>PDF</a> ]
                            [ <a href='https://github.com/happy-lifi/BCNet'>Code</a> ] <br />
                            <strong>BCNet, an general framework for effective Temporal Action Proposal
                                Generation.</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->


                    <!-- ICCV2021 -->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/ICCV2021/ASCNet.png" />
                        <div>
                            <a><b>ASCNet: Self-supervised Video Representation Learning with Appearance-Speed
                                    Consistency</b></a><br />
                            Deng Huang*, <u><b style="color:darkred">Wenhao Wu</u>*</b>, Weiwen Hu, Xu Liu, Dongliang
                            He, Zhihua Wu,
                            Xiangmiao
                            Wu, Mingkui Tan, Errui Ding <br />
                            <i>IEEE International Conference on Computer Vision <b>
                                    <font color="DarkRed">(ICCV)</font>
                                </b>, 2021 </i><br />
                            [ <a href='https://arxiv.org/pdf/2106.02342.pdf'>PDF</a> ]
                            [ <a href="papers/ICCV2021/ICCV2021_Poster.pdf">Poster</a> ]
                            [ <a href="papers/ICCV2021/iccv2021_presentation_5min_final.pdf">Slides</a> ]
                            [ <a
                                href='https://www.bilibili.com/video/BV1rg411F7MM/?spm_id_from=333.999.0.0&vd_source=b8bdec8f8b43fac4f439e3535c6aa33e'>Video</a>
                            ]
                            [ <a href=''>Code</a> ] <br />
                            <strong>An effective self-supervised video representation learning framework.</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->


                    <!-- ACMMM2021 -->
                    <!-- <div class="paper" id="xxx"><img class="paper" src="papers/ACMMM2021/DSANet.png" />
                        <div>
                            <a><b>DSANet: Dynamic Segment Aggregation Network for Video-Level Representation
                                    Learning</b></a><br />
                            <u><b style="color:darkred">Wenhao Wu</u>*</b>, Yuxiang Zhao*, Yanwu Xu, Xiao Tan, Dongliang
                            He, Zhikang
                            Zou, Jin
                            Ye, Yingying Li, Mingde Yao, Zichao Dong,
                            Yifeng Shi <br />
                            <i> ACM International Conference on Multimedia <b>
                                    <font color="DarkRed">(ACMMM)</font>
                                </b>, 2021</i> <br />
                            [ <a href='https://arxiv.org/pdf/2105.12085.pdf'>PDF</a> ]
                            [ <a href="papers/ACMMM2021/DSANet_poster.pdf">Poster</a> ]
                            [ <a href="papers/ACMMM2021/DSANet.pdf">Slides</a> ]
                            [ <a href='https://github.com/whwu95/DSANet'>Code</a> ] <br />
                            <strong>An efficient plug-and-play module for effective video-level representation
                                learning.</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->

                    <!-- AAAI2021 -->
                    <!-- <div class="paper" id="AAAI2021"><img class="paper" src="papers/AAAI2021/MVF.png" />
                        <div>
                            <a><b>MVFNet: Multi-View Fusion Network for Efficient Video Recognition</b></a><br />
                            <u><b style="color:darkred">Wenhao Wu</b></u>, Dongliang He, Tianwei Lin, Fu Li, Chuang Gan,
                            Errui Ding
                            <br />
                            <i>The AAAI Conference on Artificial Intelligence <b>
                                    <font color="DarkRed">(AAAI)</font>
                                </b>, 2021</i> <br />
                            [ <a href='https://arxiv.org/pdf/2012.06977.pdf'>PDF</a> ]
                            [ <a href="papers/AAAI2021/MVFNet_AAAI21_Poster_Wenhao.pdf">Poster</a> ]
                            [ <a href="papers/AAAI2021/mvfnet-aaai2021-presentation.pdf">Slides</a> ]
                            [ <a href='https://github.com/whwu95/MVFNet'>Code</a> ]
                            [ <a shape="rect" href="javascript:togglebib(&#39;AAAI2021&#39;)"
                                class="togglebib">Bibtex</a> ]<br />
                            <pre xml:space="preserve" style="display: none;">
          @inproceedings{wu2021mvfnet,
          title={Mvfnet: Multi-view fusion network for efficient video recognition},
          author={Wu, Wenhao and He, Dongliang and Lin, Tianwei and Li, Fu and Gan, Chuang and Ding, Errui},
          booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
          volume={35},
          number={4},
          pages={2943--2951},
          year={2021}
          }
              </pre>
                            <strong>An efficient architecture for video recognition based on 2D CNN.</strong>
                        </div>
                        <div class="spanner"></div>
                    </div> -->

                    <!-- CVPRW2021 -->
                    <!-- <div class="paper" id="CVPRW2021"><img class="paper" src="papers/CVPRW2021/aicity.png" />
                        <div>
                            <a><b>Good Practices and A Strong Baseline for Traffic Anomaly Detection</b></a><br />
                            Yuxiang Zhao*, <u><b style="color:darkred">Wenhao Wu</u>*</b>, Yue He, Yingying Li, Xiao
                            Tan, Shifeng
                            Chen <br />

                            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>
                                    <font color="DarkRed">(CVPR)</font>
                                </b> - 5th AI City Challenge (AICity), 2021</i> <br />

                            [ <a href='https://arxiv.org/abs/2105.03827'>PDF</a> ]
                            [ <a href=''>Code</a> ]
                            [ <a shape="rect" href="javascript:togglebib(&#39;CVPRW2021&#39;)"
                                class="togglebib">Bibtex</a> ]<br />
                            <pre xml:space="preserve" style="display: none;">
        @inproceedings{zhao2021good,
        title={Good Practices and A Strong Baseline for Traffic Anomaly Detection},
        author={Zhao, Yuxiang and Wu, Wenhao and He, Yue and Li, Yingying and Tan, Xiao and Chen, Shifeng},
        booktitle={Proceedings of CVPR Workshops},
        year={2021}
        }
          </pre>
                            <alert>Winner of AI City challenge for traffic anomaly detection</alert>
                        </div>
                        <div class="spanner"></div>
                    </div> -->



                    <!-- CVPRW2020 -->
                    <!-- <div class="paper" id="CVPRW2020"><img class="paper" src="papers/CVPRW2020/teaser.gif" />
                        <div>
                            <a><b>Dynamic Inference: A New Approach Toward Efficient Video Action
                                    Recognition</b></a><br />
                            <u><b style="color:darkred">Wenhao Wu</b></u>, Dongliang He, Xiao Tan, Shifeng Chen, Yi
                            Yang, Shilei Wen
                            <br />
                            <i>IEEE Conference on Computer Vision and Pattern Recognition <b>
                                    <font color="DarkRed">(CVPR)</font>
                                </b> - Joint Workshop on
                                Efficient Deep Learning in Computer Vision (EDLCV), 2020 </i><br />

                            <b>
                                <font color="Red">[Oral]</font>
                            </b>
                            [ <a
                                href='http://openaccess.thecvf.com/content_CVPRW_2020/html/w40/Wu_Dynamic_Inference_A_New_Approach_Toward_Efficient_Video_Action_Recognition_CVPRW_2020_paper.html'>PDF</a>
                            ]
                            [ <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>Slides</a> ]
                            [ <a shape="rect" href="javascript:togglebib(&#39;CVPRW2020&#39;)"
                                class="togglebib">Bibtex</a> ]<br />
                            <pre xml:space="preserve" style="display: none;">
        @inproceedings{wu2020dynamic,
            title={Dynamic Inference: A New Approach Toward Efficient Video Action Recognition},
            author={Wu, Wenhao and He, Dongliang and Tan, Xiao and Chen, Shifeng 
              and Yang, Yi and Wen, Shilei},
            booktitle={Proceedings of CVPR Workshops},
            pages={676--677},
            year={2020}
        }
            </pre>
                        </div>
                        <div class="spanner"></div>
                    </div> -->


                    <!-- ICCV2019 -->
                    <!-- <div class="paper" id="ICCV19"><img class="paper" src="papers/ICCV2019/ICCV2019.png" />
                        <div><b><a>Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video
                                    Recognition</a></b><br />
                            <u><b style="color:darkred">Wenhao Wu</b></u>, Dongliang He, Xiao Tan, Shifeng Chen, Shilei
                            Wen <br />
                            <i>IEEE International Conference on Computer Vision <b>
                                    <font color="DarkRed">(ICCV)</font>
                                </b>, 2019 </i><br /><b>
                                <font color="Red">[Oral, 4.3% acceptance rate]</font>
                            </b>
                            [ <a
                                href='http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Multi-Agent_Reinforcement_Learning_Based_Frame_Sampling_for_Effective_Untrimmed_Video_ICCV_2019_paper.pdf'>PDF</a>
                            ]
                            [ <a href="papers/ICCV2019/MARL_ICCV19_Poster_Wenhao_Wu.pdf">Poster</a> ]
                            [ <a href="papers/ICCV2019/ICCV19_Oral_5min.pdf">Slides</a> ]
                            [ <a shape="rect" href="javascript:togglebib(&#39;ICCV19&#39;)" class="togglebib">Bibtex</a>
                            ]<br />
                            <pre xml:space="preserve" style="display: none;">
@inproceedings{wu2019multi,
    title={Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed
       Video Recognition},
    author={Wu, Wenhao and He, Dongliang and Tan, Xiao and Chen, Shifeng and Wen, Shilei},
    booktitle={Proceedings of the IEEE International Conference on Computer Vision},
    pages={6222--6231},
    year={2019}
}
</pre>
                        </div>
                        <div class="spanner"></div>

                    </div>
                </div> -->




                    <!-- <div style="clear: both;">
                        <div class="section">
                            <h2>Education & Visiting</h2>
                            <div class="paper2">

                                <div class="education"><img class="education" src="logo/usyd.png" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="https://www.sydney.edu.au/">
                                                    <font color="Brown">The University of Sydney</font>
                                                </a>, Australia</b></span>
                                        <br>
                                        <p style="margin-top:6px;"><strong>Doctor of Philosophy </strong> in Computer
                                            Science
                                            <br><em>Team: <a href='https://sigmalab-usyd.github.io/'>Multimedia
                                                    Laboratory
                                                    (MMLab@USYD)</a></em>
                                            <br><em>Advisor: <a href='https://wlouyang.github.io/'>Prof. Wanli
                                                    Ouyang</a>,
                                                <a href='http://changxu.xyz/'>Prof. Chang Xu</a></em><br>
                                            2022 - 2025
                                        </p>
                                    </div>
                                </div>

                                <div class="education"><img class="education" src="logo/cuhk.jpeg" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="http://mmlab.ie.cuhk.edu.hk/">
                                                    <font color="Brown">The Chinese University of Hong Kong</font>
                                                </a>, Hong Kong</b></span>
                                        <br>
                                        <p style="margin-top:6px;"><strong> Honorary Research Assistant</strong> in
                                            <em><a href='http://mmlab.ie.cuhk.edu.hk/'>Multimedia Laboratory
                                                    (MMLab@CUHK)</a></em>
                                            <br><em>Advisor: <a href='https://wlouyang.github.io/'>Prof. Wanli
                                                    Ouyang</a></em><br>
                                            2023 - 2024
                                        </p>
                                    </div>
                                </div>

                                <div class="education"><img class="education" src="logo/ucas.jpeg" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="https://english.ucas.ac.cn/">
                                                    <font color="Brown">University of Chinese Academy of Sciences</font>
                                                </a>, China</b></span>
                                        <br>
                                        <p style="margin-top:6px;"><strong>Master of Science in Engineering</strong> in
                                            Pattern Recognition & Intelligent System
                                            <br><em>Team: <a href='http://mmlab.siat.ac.cn'>Multimedia Laboratory, SIAT,
                                                    CAS
                                                    (MMLab@SIAT-CAS)</a></em>
                                            <br><em>Advisor: <a
                                                    href='https://scholar.google.com/citations?user=6X77S3cAAAAJ&hl=en'>Prof.
                                                    Shifeng Chen</a>,
                                                <a href='http://mmlab.siat.ac.cn/yuqiao/'>Prof. Yu Qiao</a></em><br>
                                            2017 - 2020 <strong>with exam exemption (‰øùÈÄÅÁ†îÁ©∂Áîü)</strong>
                                        </p>
                                    </div>
                                </div>

                                <div class="education"><img class="education" src="logo/siat.jpg" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="http://english.siat.cas.cn/">
                                                    <font color="Brown">Shenzhen Institute of Advanced Technology,
                                                        Chinese
                                                        Academy of Science</font>
                                                </a>, China</b></span>
                                        <br>
                                        <p style="margin-top:6px;"><strong>Research Assistant</strong> in <a
                                                href='http://mmlab.siat.ac.cn'>Multimedia Laboratory
                                                (MMLab@SIAT-CAS)</a>
                                            <br><em>Advisor: <a href='http://mmlab.siat.ac.cn/sfchen'>Prof. Shifeng
                                                    Chen</a>,
                                                <a href='http://mmlab.siat.ac.cn/yuqiao/'>Prof. Yu Qiao</a></em><br>
                                            Oct. 2016 - Aug. 2017
                                        </p>
                                    </div>
                                </div>

                                <div class="education"><img class="education" src="logo/csu.png" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="https://en.csu.edu.cn/">
                                                    <font color="Brown">Central South University</font>
                                                </a>, China</b></span>
                                        <br>
                                        <p style="margin-top:6px;"><strong>Bachelor of Engineering</strong> in
                                            Automation
                                            <br><em>University Club: Electronic Design Association, Smart Vehicle
                                                Association, UAV Association</a></em><br>
                                            2013 - 2017
                                        </p>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>



                    <div style="clear: both;">
                        <div class="section">
                            <h2>Working Experience</h2>
                            <div class="paper2">

                                <div class="experience"><img class="experience" src="logo/amazon-logo.png" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="https://amazon.jobs/content/en/teams/agi">
                                                    <font color="Brown">Amazon AGI</font>
                                                </a></b></span>
                                        </br>
                                        <p style="margin-top:6px;"><strong>Applied Scientist</strong>
                                            <br><em>worked with <a href="https://davidemodolo.wordpress.com/">Dr. Davide
                                                    Modolo</a>
                                            </em>
                                            </br>
                                            Incoming, 2025. Seattle, USA
                                        </p>
                                    </div>
                                </div>

                                <div class="experience"><img class="experience" src="logo/baidu.png" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="http://vis.baidu.com/">
                                                    <font color="Brown">Baidu VIS</font>
                                                </a></b></span>
                                        <br>
                                        <p style="margin-top:6px;"><strong>Senior Researcher</strong> on Video
                                            Understanding
                                            & AIGC
                                            <br><em>worked with
                                                <a href="https://jingdongwang2017.github.io">Dr. Jingdong Wang (IEEE
                                                    Fellow)</a>,
                                                <a href="https://scholar.google.com/citations?user=ui6DYGoAAAAJ&hl=en">Dr.
                                                    Dongliang He</a> and <a
                                                    href="https://scholar.google.com/citations?hl=zh-CN&user=1wzEtxcAAAAJ">Dr.
                                                    Errui Ding</a></em><br>
                                            July 2020 - July 2023 <strong>(3 Years)</strong>. Shenzhen, China
                                        </p>
                                    </div>
                                </div>


                            </div>
                        </div>
                    </div>

                    <div style="clear: both;">
                        <div class="section">
                            <h2>Industrial Internship Experience</h2>
                            <div class="paper2">

                                <div class="experience"><img class="snap" src="logo/snap.png" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="https://research.snap.com/">
                                                    <font color="Brown">Snap Research</font>
                                                </a></b></span>
                                        </br>
                                        <p style="margin-top:6px;"><strong>Research Intern</strong> in Creative Vision
                                            Group
                                            <br><em>worked with <a
                                                    href="https://scholar.google.com/citations?hl=en&user=XUj8koUAAAAJ">Dr.
                                                    Yanyu Li</a>,
                                                <a href="https://scholar.google.com/citations?user=bZdVsMkAAAAJ&hl=en">Dr.
                                                    Anil Kag</a> and
                                                <a href="https://scholar.google.com/citations?user=mgzXR0sAAAAJ&hl=en">Dr.
                                                    Sergey Tulyakov</a>
                                            </em>
                                            </br>
                                            Mar 2025 - April 2025. Los Angeles, USA
                                        </p>
                                    </div>
                                </div>

                                <div class="experience"><img class="experience" src="logo/aws.png" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="https://aws.amazon.com/ai/">
                                                    <font color="Brown">Amazon AWS AI</font>
                                                </a></b></span>
                                        </br>
                                        <p style="margin-top:6px;"><strong>Applied Scientist Intern</strong>
                                            <br><em>worked with <a href="https://shuaizhang.tech/">Dr. Shuai Zhang</a>,
                                                <a href="https://taoyang1122.github.io/">Dr. Taojiannan Yang</a>,
                                                <a href="https://web.mit.edu/~ywang02/www/">Dr. Bernie Wang</a></em>
                                            </br>
                                            Jun. 2024 - Sep. 2024. Santa Clara, USA
                                        </p>
                                    </div>
                                </div>


                                <div class="experience"><img class="experience" src="logo/baidu.png" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="http://vis.baidu.com/">
                                                    <font color="Brown">Baidu VIS</font>
                                                </a></b></span>
                                        <br>
                                        <p style="margin-top:6px;"><strong>Research Intern</strong> on Video
                                            Understanding &
                                            AIGC
                                            <br><em>worked with <a href="https://jingdongwang2017.github.io">Dr.
                                                    Jingdong
                                                    Wang (IEEE Fellow)</a><br></em>
                                            Aug. 2023 - Present. Shenzhen / Beijing / Hybrid
                                        </p>
                                    </div>
                                </div>


                                <div class="experience"><img class="experience" src="logo/sensetime.png" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="https://www.sensetime.com/en">
                                                    <font color="Brown">SenseTime Research</font>
                                                </a></b></span>
                                        <br>
                                        <p style="margin-top:6px;"><strong>Research Intern</strong> in OpenMMLab Team
                                            <br><em>worked with <a
                                                    href="https://scholar.google.com/citations?user=eGD0b7IAAAAj">Dr.
                                                    Kai
                                                    Chen</a></em><br>
                                            Jan. 2020 - Feb. 2020. Shenzhen, China
                                        </p>
                                    </div>
                                </div>

                                <div class="experience"><img class="experience" src="logo/baidu_idl.png" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="http://vis.baidu.com/">
                                                    <font color="Brown">Baidu Research</font>
                                                </a></b></span>
                                        <br>
                                        <p style="margin-top:6px;"><strong>Research Intern</strong> in Institute of Deep
                                            Learning (IDL)
                                            <br><em>worked with <a href="https://jingdongwang2017.github.io">Dr. Xiao
                                                    Tan</a> and
                                                <a href="https://scholar.google.com/citations?user=ui6DYGoAAAAJ&hl=en">Dr.
                                                    Dongliang He</a><br></em>
                                            Sep. 2018 - Mar. 2020 <strong>(1.5 Years)</strong>. Shenzhen, China
                                        </p>
                                    </div>
                                </div>

                                <div class="experience"><img class="experience" src="logo/iqiyi.png" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="https://ir.iqiyi.com/">
                                                    <font color="Brown">iQIYI</font>
                                                </a></b></span>
                                        <br>
                                        <p style="margin-top:6px;"><strong>R&D Intern</strong> in Video Analysis Group
                                            <br><em>hosted by </a>Qiyue Liu</a></em><br>
                                            Jun. 2018 - Oct. 2018. Beijing, China
                                        </p>
                                    </div>
                                </div>

                                <div class="experience"><img class="experience" src="logo/Samsung.png" />
                                    <div>
                                        <span style="font-size: 16px;" class="h1"></b>
                                            <b><a href="https://research.samsung.com/">
                                                    <font color="Brown">Samsung Research China</font>
                                                </a></b></span>
                                        <br>
                                        <p style="margin-top:6px;"><strong>Research Intern</strong> in Samsung Advanced
                                            Institute of Technology (SAIT)
                                            <br><em>hosted by <a
                                                    href="https://www.linkedin.com/in/%E7%BD%97%E6%8C%AF%E6%B3%A2-07b971166/">Zhenbo
                                                    luo</a></em><br>
                                            Mar. 2018 - Jun. 2018. Beijing, China
                                        </p>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div> -->






                    <!-- <div style="clear: both;">
                        <div class="section">
                            <h2 id="confpapers">Contests</h2>
                            <div class="paper">
                                <ul>
                                    <li>CVPR2021 AI CITY Challenge: Traffic Anomaly Detection, <font color="Red">
                                            <b>Winner
                                                Award</b>
                                        </font>, 2021</li>
                                    <li>CVPR2021 NTIRE Challenge on Image Deblurring: Track 2 JPEG Artifacts, <font
                                            color="Blue"><b>Runner-Up Award</b></font>, 2021</li>
                                    <li>
                                        <font color="Red"><b>Meritorious Winner (First Prize)</b></font>, America
                                        Mathematical Contest in Modeling (MCM), 2016
                                    </li>
                                    <li><strong>First-class</strong> Prize, National Undergraduate Mechanical Innovation
                                        Design Competition, 2016 </li>
                                    <li>Second-class Prize, China Freescale Cup Intelligent Car Competition (South China
                                        Region), 2015 </li>
                                    <li>Second-class Prize, Smart Car Racing Competition of Hunan Province, 2015</li>



                                </ul>
                                <div class="spanner"></div>
                            </div>
                        </div>
                    </div> -->







                    <div style="clear: both;">
                        <div class="section">
                            <h2>Honors and Awards</h2>
                            <div class="paper">
                                <h3>Academic (Undergraduate/Master's/PhD) Honors</h3>
                                <li> National Scholarship awarded by the Ministry of Education (Top 0.2%), 2025</li>
                                <li> "Panshi" Scholarship, 2024</li>
                                <li> National Scholarship awarded by the Ministry of Education (Top 0.2%), 2021</li>
                                <li> Yangtze River Power Scholarship, 2020</li>
                                <!-- <h3>Professional Honors</h3>
                                <li><a href="https://good-design.org/projects/curbyit/">Australian Good Design
                                        Award</a>,
                                    2022</li>
                                <li>Baidu AIG-TPG Technical Innovation Team Award, 2021 H1</li>
                                <li>Baidu TPG-VIS Spotlight Project Award, 2021 Q1</li>
                                <li>Baidu AIG-TPG Best Innovation Team Award, 2020</li>
                                <li>Baidu Best Newcomer Award, 2020</li>
                                <li>Baidu Best Intern Award, 2019</li> -->
                            </div>
                        </div>
                    </div>


                    <!-- <div style="clear: both;">
                        <div class="section">
                            <h2>Academic Activities</h2>
                            <div class="paper">
                                <h3>Journal Reviewer</h3>
                                <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>)</li>
                                <li>IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS</b>)</li>
                                <li>IEEE Transaction on Image Processing (<b>TIP</b>)</li>
                                <li>IEEE Transaction on Circuits and Systems for Video Technology (<b>TCSVT</b>)</li>
                                <li>IEEE Transactions on Multimedia (<b>TMM</b>)</li>
                                <li>Computer Vision and Image Understanding (<b>CVIU</b>)</li>
                                <li>IEEE Transactions on Biomedical Engineering (<b>TBME</b>)</li>
                                <li>Knowledge-Based Systems (<b>KBS</b>)</li>
                                <li>International Journal of Multimedia Information Retrieval (<b>IJMIR</b>)</li>
                                <li>IEEE Intelligent Transportation Systems Transactions (<b>TITS</b>)</li>
                                <li>Internet of Things (<b>IOT</b>)</li>
                                <li>Transactions on Multimedia Computing Communications and Applications (<b>TOMM</b>)
                                </li>

                                <h3>Conference PC Member/Reviewer</h3>
                                <li>Reviewer, International Conference on Machine Learning (<b>ICML</b>), 2025</li>
                                <li>Reviewer, Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2024
                                </li>
                                <li>Reviewer, The Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>),
                                    2021,
                                    2022, 2023, 2024, 2025</li>
                                <li>Reviewer, International Conference on Computer Vision (<b>ICCV</b>), 2023, 2025</li>
                                <li>Reviewer, European Conference on Computer Vision (<b>ECCV</b>), 2022, 2024</li>
                                <li>PC Member, International Joint Conference on Artificial Intelligence (<b>IJCAI</b>),
                                    2021</li>
                                <li>PC Member, The AAAI Conference on Artifical Intelligence (<b>AAAI</b>), 2021, 2022,
                                    2023
                                </li>
                                <li>Reviewer, ACM International Conference on Multimedia (<b>ACMMM</b>), 2023, 2024</li>
                                <li>Reviewer, Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2022
                                </li>
                                <li>Reviewer, International Conference on Digital Image Computing: Techniques and
                                    Applications (<b>DICTA</b>), 2022</li>

                                <h4>Member of IEEE, ACM, AAAI and CVF</h4>
                                <h4>Off-Campus Mentor of Tsinghua University, 2023-2025</h4>

                            </div>
                        </div>
                    </div> -->


                    <!-- <div style="clear: both;">
                        <div class="section">
                            <h2>Mentoring Experience</h2>
                            <div class="paper">
                                <h3>I am fortunate to have provided help to talented mentees (at Baidu/University)
                                    including:</h3> </br>
                                <h3>Mentored Interns at Baidu (‚Üí: Post-mentorship status)</h3>
                                <a href='https://openreview.net/profile?id=~Huanjin_Yao1'>Huanjin Yao</a> (Master,
                                Tsinghua
                                University ‚Üí PhD, NTU), <strong>NeurIPS'24</strong>, 2023 ‚Üù Now </br>
                                <a href='https://bofang98.github.io/'>Bo Fang</a> (Master, CAS ‚Üí PhD, CityU),
                                <strong>ACMMM'23 Oral, ICCV'23, CVPR'25</strong>, 2021 ‚Üù Now </br>
                                <a href="https://scholar.google.com/citations?user=73tAoEAAAAAJ&hl=en">Mengxi
                                    Zhang</a>(Master, Tianjin University ‚Üí ByteDance AI Lab),
                                <strong>NeurIPS'24</strong>,
                                2024 </br>
                                <a href='https://lawrencexia2008.github.io'>Boyang Xia</a> (Master, CAS ‚Üí Kuaishou),
                                <strong>2x ECCV'22</strong>, 2021 ‚Üù 2022 </br>
                                <a href='https://scholar.google.com/citations?user=H8uoAdMAAAAJ&hl=en'>Haosen Yang</a>
                                (Master, HIT ‚Üí PhD, University of Surrey), <strong>AAAI'22</strong>, 2021 </br>
                                <a href='https://scholar.google.com.hk/citations?user=rSuLEXwAAAAJ&hl=en'>Deng Huang</a>
                                (Master, SCUT ‚Üí AutoX), <strong>ICCV'21</strong>, 2021 </br>
                                <a href=''>Yuguo Wang</a> (Bachelor, UESTC ‚Üí Master, Duke University), 2022 </br>
                                <a href='https://zhaohengyuan1.github.io/'>Hengyuan Zhao</a> (Bachelor, NUPT ‚Üí PhD,
                                NUS),
                                2021 </br>
                                <a href='https://scholar.google.com/citations?user=lyUIclsAAAAJ&hl=en'>Yuxiang Zhao</a>
                                (Master, CAS ‚Üí PhD, Peking University, <strong>ACMMM'21, CVPR'21 AICity
                                    WinnerüèÜ</strong>),
                                2021 </br>

                                <br>
                                <h3>University Mentees</h3>
                                <a href='https://scholar.google.com.hk/citations?user=ysul-AUAAAAJ&hl=EN'>Guangzhao
                                    Dai</a>
                                (PhD, NJUST), <strong>TMM'24</strong>, 2023</br>
                                <a href='https://openreview.net/profile?id=~Haipeng_Luo2'>Haipeng Luo</a> (Master, CAS ‚Üí
                                PhD, Tsinghua University), <strong>CVPR'23</strong>, 2022 ‚Üù 2023 </br>
                                <a href=''>Zhihao Wang</a> (Master, CAS ‚Üí MiniMax), <strong>ECCV'22</strong>, 2022 </br>
                            </div>
                        </div>
                    </div>

                    <div style="clear: both;">
                        <div class="section">
                            <h2>Collaborators & Friends</h2>
                            <div class="paper">
                                <a href='https://wxh1996.github.io/'>Xiaohan Wang</a> (Stanford University),
                                <a href='https://www.linkedin.com/in/xiao-tan-46b70a85/'>Xiao Tan</a> (Baidu),
                                <a href='https://www.linkedin.com/in/dongliang-he-6b926077'>Dongliang He</a>
                                (ByteDance),
                                <a href='https://wzmsltw.github.io/'>Tianwei Lin</a> (Horizon),
                                <a href='https://xuyanwu.github.io/'>Yanwu Xu</a> (Boston University),
                                <a href='https://wujie1010.github.io/'>Jie Wu</a> (ByteDance),
                                <a href='https://yejin0111.github.io/'>Jin Ye</a> (Monash University),
                                <a href='https://people.csail.mit.edu/ganchuang/'>Chuang Gan</a> (MIT-IBM Watson Lab),
                                <a href='https://scholar.google.com.hk/citations?user=WRIYcNwAAAAJ&hl=zh-CN'>Yihao
                                    Liu</a>
                                (Shanghai Lab),
                                <a href='https://scholar.google.co.jp/citations?user=Y-3iZ9EAAAAJ&hl=en'>Zhun Sun</a>
                                (Tohoku University, Japan),
                                <a href="https://mdyao.github.io/">Mingde Yao</a> (CUHK),
                                <a href="https://scholar.google.com/citations?user=l_e7BDEAAAAJ&hl=en&oi=sra">Min
                                    Yang</a>
                                (ByteDance)





                            </div>
                        </div>
                    </div> -->


                    <div style="clear:both;">
                        <p align="right">
                            <font size="5">Last Updated on 11th Oct, 2025</a></font>
                        </p>
                        <p align="right">
                            <font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font>
                        </p>
                    </div>

                    <!-- <hr> -->
                    <!-- <div id="clustrmaps-widget"></div> -->
                    <!-- <script type='text/javascript' id='clustrmaps'
                        src='//cdn.clustrmaps.com/map_v2.js?cl=0e1633&w=300&t=tt&d=N0ZXDEaZVrn2LXkG_byNAa2NLm2v6WRQIUifhg-2f1A&co=0b4975&ct=cdd4d9&cmo=3acc3a&cmn=ff5353'></script> -->

</body>

</html>