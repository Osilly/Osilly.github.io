{% comment %} <html>

<head>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <title>Publications</title>
    <meta content="Publications, whwu95.github.io" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h3,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: Arial, Helvetica, sans-serif;
            /* font-family: Lato, sans-serif; */
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #043d98;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }



        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 2em auto 2em auto;
            width: 870px;
            font-family: Open Sans Light, Helvetica, sans-serif;
            font-size: 14px;
            background: #F4F6F6;
        }

        h2 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        h4 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700;
        }

        strong {
            /* font-family: Lato, Verdana, Helvetica, sans-serif; */
            /* font-size: 13px; */
            font-weight: bold;
        }

        ul {
            /* list-style: circle; */
            list-style: disc;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Arial, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.2em;
            background: #F4F6F6;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.4em;
            margin-bottom: 0.7em;
            border: 2px solid #ddd;
            background: #fff;
            padding: 0.55em .8em 0.6em .8em;
            border-top-right-radius: 10px;
            border-top-left-radius: 10px;
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
            line-height: 140%;
        }

        div.paper2 {
            clear: both;
            margin-top: 0.4em;
            margin-bottom: 0.7em;
            border: 0px solid #ddd;
            background: #fff;
            padding: 0.55em .8em 0.6em .8em;
            border-top-right-radius: 10px;
            border-top-left-radius: 10px;
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
            line-height: 140%;
        }

        div.paper:hover {
            background: #FFFDEE;
            /* background-color: #242d36 ; */
        }

        div.paper2:hover {
            background: #FFFDEE;
            /* background-color: #242d36 ; */
        }

        div.bio {
            clear: both;
            margin-top: 0.4em;
            margin-bottom: 0.7em;
            border: 0px solid #ddd;
            background: #fff;
            padding: 0.55em .8em 0.6em .8em;
            border-top-right-radius: 10px;
            border-top-left-radius: 10px;
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
            line-height: 135%;
        }

        div.res {
            clear: both;
            margin-top: 0.4em;
            margin-bottom: 0.4em;
            border: 0px solid #ddd;
            background: #fff;
            padding: 0.65em .8em 0.15em .8em;
            border-top-right-radius: 10px;
            border-top-left-radius: 10px;
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
            line-height: 130%;
        }

        div.award {
            clear: both;
            margin-top: 0.4em;
            margin-bottom: 0.4em;
            border: 0px solid #ddd;
            background: #fff;
            padding: 0.65em .8em 0.15em .8em;
            border-top-right-radius: 10px;
            border-top-left-radius: 10px;
            border-bottom-left-radius: 10px;
            border-bottom-right-radius: 10px;
            line-height: 130%;
        }

        div.paper div {
            padding-left: 270px;
        }

        img.paper {
            /* margin-bottom: 0.4em; */
            float: left;
            width: 250px;

        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: Open Sans Light, Helvetica, sans-serif;
            font-size: 14px;
            margin: 1em 0;
            padding: 0;
        }

        .bot {
            font-size: 14%;
        }

        .ptypej {
            display: inline;
            padding: .0em .2em .05em;
            font-size: 85%;
            font-weight: bold;
            line-height: 1;
            background-color: #5cb85c;
            color: #FFFFFF;
            text-align: center;
            white-space: nowrap;
            vertical-align: baseline;
            margin-right: 6px;
        }

        .ptypec {
            display: inline;
            padding: .0em .2em .05em;
            font-size: 85%;
            font-weight: bold;
            line-height: 1;
            background-color: #428bca;
            color: #FFFFFF;
            text-align: center;
            white-space: nowrap;
            vertical-align: baseline;
            margin-right: 6px;
        }

        .ptypep {
            display: inline;
            padding: .0em .2em .05em;
            font-size: 85%;
            font-weight: bold;
            line-height: 1;
            background-color: #6B6B6B;
            color: #FFFFFF;
            text-align: center;
            white-space: nowrap;
            vertical-align: baseline;
            margin-right: 6px;
        }

        /* navigation */
        #nav {
            /* font-family: 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans',*/
            /* Corbel, Arial, Helvetica, sans-serif; */
            font-family: Georgia, Helvetica, sans-serif;
            position: fixed;
            top: 50px;
            /* left: 860px; */
            margin-left: 860px;
            /*1060*/
            width: 92px;
            font-size: 15px;
        }

        #nav li2 {
            margin-bottom: 1px;
        }

        ol {
            padding-left: 10px;
            /* list-style: none; */
        }

        #nav a {
            display: block;
            padding: 6px 9px 7px;
            color: #fff;
            background-color: #455A64;
            text-decoration: none;
        }

        #nav a:hover {
            color: #ffde00;
            /* background-color: #242d36 ; */
        }
    </style>

    <!-- <script type="text/javascript" async="" src="./files/ga.js"></script>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-7953909-1']);
  _gaq.push(['_trackPageview']);

  (function () {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script> -->

    <script type="text/javascript" async=""
        src="https://www.googletagmanager.com/gtag/js?id=G-BVM4SSY0TG&amp;cx=c&amp;_slc=1"></script>
    <script async="" src="//www.google-analytics.com/analytics.js"></script>
    <script async="" src="//www.google-analytics.com/analytics.js"></script>
    <script type="text/javascript" src="./files/hidebib.js"></script>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
        type="text/css">
    <!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>-->
    <!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>-->
    <!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');



    </script>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-66888300-1', 'auto');
        ga('send', 'pageview');

    </script>
</head>



<!-- <script src="./files/main.js"></script> -->

<body>
    <div style="clear: both;">
        <div class="section">
            <h2 id="confpapers">Publications</h2>



            <div class="paper">
                <h4>Conferences: ICCV (5), CVPR (5), ECCV (5), NeurIPS (4), AAAI (4), IJCAI (1), ACMMM (3) -- Oral x2,
                    Highlight x2, Spotlight x1</h4>
                <h4>Journals: TPAMI (1), IJCV (1), TMM (1)</h4>
                <h4>( *Co-first Author, <sup><span style="font-family:Wingdings">*</span></sup>Corresponding Author)
                </h4>
                <br>

                <h3>Preprints:</h3><br>
                <ol>







                    <li><b>FreeVA: Offline MLLM as Training-Free Video Assistant</b>
                        <br>
                        <u><b>Wenhao Wu</b></u><br>
                        <em>
                            <font color="#B71C1C">Technical Report, ArXiv:2405.07798</font>
                        </em>&nbsp;&nbsp;
                        [ <a href="https://arxiv.org/pdf/2405.07798">PDF</a> ]
                        [ <a href="https://github.com/whwu95/FreeVA">Code</a> ]
                        <br>
                    </li>





                    <li><b>GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?</b>
                        <br>
                        <u><b>Wenhao Wu</b></u>, Huanjin Yao, Mengxi Zhang, Yuxin Song, Wanli Ouyang, Jingdong Wang<br>
                        <em>
                            <font color="#B71C1C">Technical Report, ArXiv:2311.15732</font>
                        </em>&nbsp;&nbsp;
                        [ <a href="https://arxiv.org/pdf/2311.15732.pdf">PDF</a> ]
                        [ <a href="https://github.com/whwu95/GPT4Vis">Code</a> ]
                        <br>
                    </li>

                    <li><b>Side4Video: Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer
                            Learning</b>
                        <br>
                        Huanjin Yao*, <u><b>Wenhao Wu</b></u><b>*<sup><span
                                    style="font-family:Wingdings">*</span></sup></b>, Zhiheng Li<br>
                        <em>
                            <font color="#B71C1C">Technical Report, ArXiv:2311.15769</font>
                        </em>&nbsp;&nbsp;
                        [ <a href="https://arxiv.org/pdf/2311.15769.pdf">PDF</a> ]
                        [ <a href="https://github.com/HJYao00/Side4Video">Code</a> ]
                        <br>
                    </li>

                    <li><b>It Takes Two: Masked Appearance-Motion Modeling for Self-supervised Video Transformer
                            Pre-training</b> <br>
                        Yuxin Song, Min Yang, <u><b>Wenhao Wu</b></u>, Dongliang He, Fu Li, Jingdong Wang<br>
                        <em>
                            <font color="#B71C1C">Technical Report, ArXiv:2210.05234</font>
                        </em>&nbsp;&nbsp;
                        [ <a href="https://arxiv.org/pdf/2210.05234.pdf">PDF</a> ]
                        <br>
                    </li>

                    <li><b>Discovering “Semantics” in Super-Resolution Networks</b> <br>
                        Yihao Liu, Anran Liu, Jinjin Gu, Zhipeng Zhang, <u><b>Wenhao Wu</b></u>,
                        Yu Qiao, Chao Dong <br>
                        <em>
                            <font color="#B71C1C">Technical Report, ArXiv:2108.00406</font>
                        </em>&nbsp;&nbsp;
                        [ <a href="https://arxiv.org/pdf/2108.00406.pdf">PDF</a> ]
                        [ <a href="">Code</a> ]
                        <br>
                    </li>

                    <li><b>Color2Style: Real-Time Exemplar-Based Image Colorization with Self-Reference Learning and
                            Deep Feature Modulation</b> <br>
                        Henyuan Zhao*, <u><b>Wenhao Wu</b></u><b>*</b>, Yihao Liu, Dongliang He <br>
                        <em>
                            <font color="#B71C1C">Technical Report, ArXiv:2106.08017</font>
                        </em>&nbsp;&nbsp;
                        [ <a href="https://arxiv.org/pdf/2106.08017.pdf">PDF</a> ]
                        [ <a href="https://github.com/zhaohengyuan1/Color2Style">Code</a> ]
                        <br>
                    </li>


                    <li><b>Temporal Action Proposal Generation with Transformers</b> <br>
                        Lining Wang*, Haosen Yang*, <u><b>Wenhao Wu</b></u><b>*</b>, Hongxun Yao, Hujie
                        Huang <br>
                        <em>
                            <font color="#B71C1C">Technical Report, ArXiv:2105.12043</font>
                        </em>&nbsp;&nbsp;
                        [ <a href="https://arxiv.org/pdf/2105.12043.pdf">PDF</a> ]
                        <br>
                    </li>
                    <br>


                    <h3>Journal Papers:</h3><br>
                    <ol>

                        <li><b>Cap4Video++: Enhancing Video Understanding with Auxiliary Captions</b>
                            <br>
                            <u><b>Wenhao Wu</b></u>, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, Wanli Ouyang<br>
                            <em>
                                <font color="#B71C1C">Transactions on Pattern Analysis and Machine Intelligence (TPAMI),
                                    2024.</font>
                            </em>&nbsp;&nbsp;
                            <alert>Impact factor: 23.6</alert>&nbsp;&nbsp;
                            [ <a href="https://ieeexplore.ieee.org/document/10670217">PDF</a> ]
                            [ <a href="https://github.com/whwu95/Cap4Video">Code</a> ]
                            <br>
                        </li>

                        <li><b>Transferring Vision-Language Models for Visual Recognition: A Classifier Perspective</b>
                            <br>
                            <u><b>Wenhao Wu</b></u>, Zhun Sun, Yuxin Song, Jingdong Wang, Wanli Ouyang<br>
                            <em>
                                <font color="#B71C1C">International Journal of Computer Vision (IJCV), 2023.</font>
                            </em>&nbsp;&nbsp;
                            <alert>Impact factor: 19.5</alert>&nbsp;&nbsp;
                            [ <a href="https://link.springer.com/article/10.1007/s11263-023-01876-w">PDF</a> ]
                            [ <a href="https://github.com/whwu95/Text4Vis">Code</a> ]
                            <br>
                        </li>

                        <li><b>GPT4Ego: Unleashing the Potential of Pre-trained Models for Zero-Shot Egocentric Action
                                Recognition</b>
                            <br>
                            Guangzhao Dai, Xiangbo Shu, <u><b>Wenhao Wu</b></u>, Rui Yan, Jiachao Zhang<br>
                            <em>
                                <font color="#B71C1C">Transactions on Multimedia, 2024</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2401.10039.pdf">PDF</a> ]
                            <br>
                        </li>

                        <li><b>Rethinking 3D cost aggregation in stereo matching</b>
                            <br>
                            Wanshui Gan, <u><b>Wenhao Wu</b></u>, Shifeng Chen, Yuxiang Zhao, Pak Kin Wong<br>
                            <em>
                                <font color="#B71C1C">Pattern Recognition Letters, 2023</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://dl.acm.org/doi/abs/10.1016/j.patrec.2023.02.011">PDF</a> ]
                            [ <a href="https://github.com/GANWANSHUI/HybridNet">Code</a> ]
                            <br>
                        </li>

                    </ol>
                    <br>


                    <h3>Conference Papers:</h3><br>
                    <ol>

                        <li><b>Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte
                                Carlo Tree Search</b>
                            <br>
                            Huanjin Yao, Jiaxing Huang, <u><b>Wenhao Wu</b></u>, Jingyi Zhang, Yibo Wang, Shunyu Liu,
                            Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, Dacheng Tao
                            <br>
                            <em>
                                <font color="#B71C1C">NeurIPS 2025</font>
                            </em>&nbsp;&nbsp;<em>
                                <alert>Spotlight</alert>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2412.18319">PDF</a> ]
                            [ <a href="https://github.com/HJYao00/Mulberry">Code</a> ]
                            <br>
                        </li>


                        <li><b>R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via
                                Share-GRPO</b>
                            <br>
                            Huanjin Yao, Qixiang Yin, Jingyi Zhang, Min Yang, Yibo Wang, <u><b>Wenhao Wu</b></u>, Fei
                            Su, Li Shen, Minghui Qiu, Dacheng Tao, Jiaxing Huang
                            <br>
                            <em>
                                <font color="#B71C1C">NeurIPS 2025</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2505.16673">PDF</a> ]
                            [ <a href="https://github.com/HJYao00/R1-ShareVL">Code</a> ]
                            <br>
                        </li>

                        <li><b>MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward
                                AGI</b>
                            <br>
                            Huanjin Yao, Jiaxing Huang, Yawen Qiu, Michael K. Chen, Wenzheng Liu, Wei Zhang, Wenjie
                            Zeng, Xikun Zhang, Jingyi Zhang, Yuxin Song, <u><b>Wenhao Wu</b></u>, Dacheng Tao
                            <br>
                            <em>
                                <font color="#B71C1C">ICCV 2025</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/abs/2506.23563">PDF</a> ]
                            [ <a href="https://github.com/HJYao00/MMReason">Code</a> ]
                            <br>
                        </li>


                        <li><b>DistinctAD: Distinctive Audio Description Generation in Contexts</b>
                            <br>
                            Bo Fang, <u><b>Wenhao Wu</b></u>, Qiangqiang Wu, Yuxin Song, Antoni B. Chan<br>
                            <em>
                                <font color="#B71C1C">CVPR 2025
                                </font>
                            </em>&nbsp;&nbsp;<em>
                                <alert>Highlight</alert>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2411.18180">PDF</a> ]
                            <br>
                        </li>

                        <li><b>Dense Connector for MLLMs</b>
                            <br>
                            Huanjin Yao*, <u><b>Wenhao Wu</b></u><b>*<sup><span
                                        style="font-family:Wingdings">*</span></sup></b>, Taojiannan Yang, Yuxin Song,
                            Mengxi Zhang,
                            Haocheng Feng,
                            Yifan Sun,
                            Zhiheng Li,
                            Wanli Ouyang,
                            Jingdong Wang<br>
                            <em>
                                <font color="#B71C1C">NeurIPS 2024</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2405.13800">PDF</a> ]
                            [ <a href="https://github.com/HJYao00/DenseConnector">Code</a> ]
                            <br>
                        </li>

                        <li><b>Automated Multi-level Preference for MLLMs</b>
                            <br>
                            Mengxi Zhang, <u><b>Wenhao Wu</b></u>, Yu Lu, Yuxin Song, Kang Rong, Huanjin Yao, Jianbo
                            Zhang, Fanglong Liu, Yifan Sun, Haocheng Feng, Jingdong Wang<br>
                            <em>
                                <font color="#B71C1C">NeurIPS 2024</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2405.11165">PDF</a> ]
                            [ <a href="https://github.com/takomc/amp">Code</a> ]
                            <br>
                        </li>


                        <li><b>DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM</b>
                            <br>
                            Yixuan Wu, Yizhou Wang, Shixiang Tang, <u><b>Wenhao Wu</b></u>, Tong He, Wanli Ouyang, Jian
                            Wu, Philip Torr<br>
                            <em>
                                <font color="#B71C1C">ECCV 2024</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2403.12488.pdf">PDF</a> ]
                            <br>
                        </li>

                        <li><b>What Can Simple Arithmetic Operations Do for Temporal Modeling?</b>
                            <br>
                            <u><b>Wenhao Wu</b></u>, Yuxin Song, Zhun Sun, Jingdong Wang, Chang Xu, Wanli Ouyang<br>
                            <em>
                                <font color="#B71C1C">ICCV 2023</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2307.08908.pdf">PDF</a> ]
                            [ <a href="https://github.com/whwu95/ATM">Code</a> ]
                            <br>
                        </li>

                        <li><b>UATVR: Uncertainty-Adaptive Text-Video Retrieval</b>
                            <br>
                            Bo Fang*, <u><b>Wenhao Wu*</b></u>, Chang Liu*, Yu Zhou, Yuxin Song,
                            Weiping Wang, Xiangbo Shu, Xiangyang Ji, Jingdong Wang<br>
                            <em>
                                <font color="#B71C1C">ICCV 2023</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2301.06309.pdf">PDF</a> ]
                            [ <a href="https://github.com/bofang98/UATVR">Code</a> ]
                            <br>
                        </li>

                        <li><b>Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?</b>
                            <br>
                            <u><b>Wenhao Wu</b></u>, Haipeng Luo, Bo Fang, Jingdong Wang, Wanli Ouyang<br>
                            <em>
                                <font color="#B71C1C">CVPR 2023
                                </font>
                            </em>&nbsp;&nbsp;<em>
                                <alert>Highlight, 2.5% acceptance rate</alert>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2301.00184.pdf">PDF</a> ]
                            [ <a href="https://github.com/whwu95/Cap4Video">Code</a> ]
                            <br>
                        </li>


                        <li><b>Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained
                                Vision-Language Models</b> <br>
                            <u><b>Wenhao Wu</b></u>, Xiaohan Wang, Haipeng Luo, Jingdong Wang, Yi Yang, Wanli Ouyang<br>
                            <em>
                                <font color="#B71C1C">CVPR 2023
                                </font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2301.00182.pdf">PDF</a> ]
                            [ <a href="https://github.com/whwu95/BIKE">Code</a> ]
                            <br>
                        </li>





                        <li><b>Revisiting Classifier: Transferring Vision-Language Models for Video Recognition</b> <br>
                            <u><b>Wenhao Wu</b></u>, Zhun Sun, Wanli Ouyang<br>
                            <em>
                                <font color="#B71C1C">AAAI 2023
                                </font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2207.01297.pdf">PDF</a> ]
                            [ <a href="https://github.com/whwu95/Text4Vis">Code</a> ]
                            [ <a href="papers/AAAI2023/text4vis_AAAI23_Poster_Wenhao.pdf">Poster</a> ]
                            [ <a href="papers/AAAI2023/text4vis-aaai2023-presentation.pdf">Slides</a> ]
                            [ <a href="">Video</a> ]
                            <br>
                        </li>

                        <li><b>AdaCM: Adaptive ColorMLP for Real-Time Universal Photo-realistic Style Transfer</b> <br>
                            Tianwei Lin, Honglin Lin, Fu Li, Dongliang He, <u><b>Wenhao Wu</b></u>, Meiling Wang, Xin
                            Li, Yong Liu<br>
                            <em>
                                <font color="#B71C1C">AAAI 2023
                                </font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2212.01567.pdf">PDF</a> ]
                            <br>
                        </li>

                        <li><b>Effective Invertible Arbitrary Image Rescaling</b> <br>
                            Zhihong Pan, Baopu Li, Dongliang He, <u><b>Wenhao Wu</b></u>, Errui Ding<br>
                            <em>
                                <font color="#B71C1C">WACV 2023
                                </font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2209.13055.pdf">PDF</a> ]
                            [ <a href="">Code</a> ]
                            <br>
                        </li>

                        <li><b>NSNet: Non-saliency Suppression Sampler for Efficient Video Recognition</b> <br>
                            Boyang Xia*, <u><b>Wenhao Wu</b></u><b>*<sup><span
                                        style="font-family:Wingdings">*</span></sup></b>, Haoran Wang, Rui Su, Dongliang
                            He, Haosen Yang, Xiaoran Fan, Wanli Ouyang <br>
                            <em>
                                <font color="#B71C1C">ECCV 2022
                                </font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2207.10388.pdf">PDF</a> ]
                            [ <a href="https://lawrencexia2008.github.io/projects/nsnet">Project</a> ]
                            <br>
                        </li>


                        <li><b>Temporal Saliency Query Network for Efficient Video Recognition</b> <br>
                            Boyang Xia*, Zhihao Wang*, <u><b>Wenhao Wu</b></u><b><sup><span
                                        style="font-family:Wingdings">*</span></sup></b>, Haoran Wang, Jungong Han <br>
                            <em>
                                <font color="#B71C1C">ECCV 2022
                                </font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2207.10379.pdf">PDF</a> ]
                            [ <a href="https://lawrencexia2008.github.io/projects/tsqnet">Project</a> ]
                            <br>
                        </li>

                        <li><b>CODER: Coupled Diversity-Sensitive Momentum Contrastive Learning for Image-Text
                                Retrieval</b> <br>
                            Haoran Wang, Dongliang He, <u><b>Wenhao Wu</b></u>, Boyang Xia, Min Yang, Fu Li, Yunlong Yu,
                            Zhong Ji, Errui Ding, Jingdong Wang <br>
                            <em>
                                <font color="#B71C1C">ECCV 2022
                                </font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2208.09843.pdf">PDF</a> ]
                            [ <a href="">Code</a> ]
                            <br>
                        </li>


                        <li><b>MaMiCo: Macro-to-Micro Semantic Correspondence for Self-supervised Video Representation
                                Learning</b> <br>
                            Bo Fang*, <u><b>Wenhao Wu</b></u><b>*</b>, Chang Liu*, Yu Zhou, Dongliang He, Weiping Wang
                            <br>
                            <em>
                                <font color="#B71C1C">ACMMM 2022
                                </font>
                            </em>&nbsp;&nbsp;<em>
                                <alert>Oral, 5.0% acceptance rate</alert>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://dl.acm.org/doi/10.1145/3503161.3547888">PDF</a> ]
                            [ <a href="">Code</a> ]
                            <br>
                        </li>



                        <li><b>Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation</b> <br>
                            Yanwu Xu, Shaoan Xie, <u><b>Wenhao Wu</b></u>, Kun Zhang, Mingming Gong, Kayhan
                            Batmanghelich <br>
                            <em>
                                <font color="#B71C1C">CVPR 2022
                                </font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2203.12707.pdf">PDF</a> ]
                            [ <a href="https://github.com/batmanlab/MSPC">Code</a> ]
                            <br>
                        </li>

                        <li><b>Towards Bidirectional Arbitrary Image Rescaling: Joint Optimization and Cycle
                                Idempotence</b> <br>
                            Zhihong Pan, Baopu Li, Dongliang He, Mingde Yao, <u><b>Wenhao Wu</b></u>, Tianwei Lin, Xin
                            Li, Errui Ding <br>
                            <em>
                                <font color="#B71C1C">CVPR 2022
                                </font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2203.00911.pdf">PDF</a> ]
                            [ <a href="">Code</a> ]
                            <br>
                        </li>


                        <li><b>Temporal Action Proposal Generation with Background Constraint</b> <br>
                            Haosen Yang*, <u><b>Wenhao Wu</b></u><b>*</b>, Lining Wang, Sheng Jin,
                            Boyang Xia, Hongxun Yao, Hujie Huang <br>
                            <em>
                                <font color="#B71C1C">AAAI 2022
                                </font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2112.07984.pdf">PDF</a> ]
                            [ <a href="https://github.com/happy-lifi/BCNet">Code</a> ]
                            <br>
                        </li>



                        <li><b>ASCNet: Self-supervised Video Representation Learning with Appearance-Speed
                                Consistency</b> <br>
                            Deng Huang*, <u><b>Wenhao Wu</b></u><b>*</b>, Weiwen Hu, Xu Liu, Dongliang He,
                            Zhihua Wu, Xiangmiao Wu, Mingkui Tan, Errui Ding <br>
                            <em>
                                <font color="#B71C1C">ICCV 2021</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2106.02342.pdf">PDF</a> ]
                            [ <a href="papers/ICCV2021/ICCV2021_Poster.pdf">Poster</a> ]
                            [ <a href="papers/ICCV2021/iccv2021_presentation_5min_final.pdf">Slides</a> ]
                            [ <a href="">Video</a> ]
                            [ <a href="">Code</a> ]
                            <br>
                        </li>


                        <li><b>DSANet: Dynamic Segment Aggregation Network for Video-Level Representation
                                Learning</b> <br>
                            <u><b>Wenhao Wu</b></u>, Yuxiang Zhao, Yanwu Xu, Xiao Tan, Dongliang He,
                            Zhikang Zou, Jin Ye, Yingying Li, Mingde Yao, Zichao Dong,
                            Yifeng Shi <br>
                            <em>
                                <font color="#B71C1C">ACMMM 2021</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2105.12085.pdf">PDF</a> ]
                            [ <a href="papers/ACMMM2021/DSANet_poster.pdf">Poster</a> ]
                            [ <a href="papers/ACMMM2021/DSANet.pdf">Slides</a> ]
                            [ <a href="https://github.com/whwu95/DSANet">Code</a>]
                            <br>
                        </li>


                        <li><b>Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network</b> <br>
                            Zhikang Zou, Xiaoye Qu, Pan Zhou, Shuangjie Xu, Xiaoqing Ye, <u><b>Wenhao Wu</b></u>, Jin Ye
                            <br>
                            <em>
                                <font color="#B71C1C">ACMMM 2021</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2107.12858.pdf">PDF</a> ]
                            <br>
                        </li>




                        <li><b>Weakly-Supervised Spatio-Temporal Anomaly Detection in Surveillance Video</b> <br>
                            Jie Wu, Wei Zhang, Guanbin Li, <u><b>Wenhao Wu</b></u>, Xiao Tan, Yingying Li, Errui Ding,
                            Liang Lin <br>
                            <em>
                                <font color="#B71C1C">IJCAI 2021</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2108.03825.pdf">PDF</a> ]
                            <br>
                        </li>


                        <li><b>Good Practices and A Strong Baseline for Traffic Anomaly Detection</b> <br>
                            Yuxiang Zhao*, <u><b>Wenhao Wu</b></u><b>*</b>, Yue He, Yingying Li, Xiao Tan,
                            Shifeng Chen <br>
                            <em>
                                <font color="#B71C1C">CVPR 2021</font>&nbsp;&nbsp;&nbsp;<font color="Gray">Workshop on
                                    AICity Challenge</font>&nbsp;&nbsp;
                                <alert>Winner</alert>
                            </em>
                            [ <a href="https://arxiv.org/abs/2105.03827">PDF</a> ]
                            <br>
                        </li>


                        <li><b>MVFNet: Multi-View Fusion Network for Efficient Video Recognition</b> <br>
                            <u><b>Wenhao Wu</b></u>, Dongliang He, Tianwei Lin, Fu Li, Chuang Gan,
                            Errui Ding <br>
                            <em>
                                <font color="#B71C1C">AAAI 2021</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="https://arxiv.org/pdf/2012.06977.pdf">PDF</a> ]
                            [ <a href="papers/AAAI2021/MVFNet_AAAI21_Poster_Wenhao.pdf">Poster</a> ]
                            [ <a href="papers/AAAI2021/mvfnet-aaai2021-presentation.pdf">Slides</a> ]
                            [ <a href="https://github.com/whwu95/MVFNet">Code</a> ]
                            <br>
                        </li>


                        <li><b>Attention-Driven Dynamic Graph Convolutional Network for Multi-Label Image
                                Recognition</b> <br>
                            Jin Ye, Junjun He, Xiaojiang Peng, <u><b>Wenhao Wu</b></u>, Yu Qiao <br>
                            <em>
                                <font color="#B71C1C">ECCV 2020</font>
                            </em>&nbsp;&nbsp;
                            [ <a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660647.pdf">PDF</a> ]
                            [ <a href="https://github.com/Yejin0111/ADD-GCN">Code</a> ]
                            <br>
                        </li>


                        <li><b>Dynamic Inference: A New Approach Toward Efficient Video Action Recognition</b> <br>
                            <u><b>Wenhao Wu</b></u>, Dongliang He, Xiao Tan, Shifeng Chen, Yi Yang,
                            Shilei Wen <br>
                            <em>
                                <font color="#B71C1C">CVPR 2020</font>&nbsp;&nbsp;&nbsp;<font color="Gray">Workshop on
                                    Efficient Deep Learning in Computer Vision</font>&nbsp;&nbsp;<alert>Oral</alert>
                            </em>&nbsp;&nbsp;
                            [ <a
                                href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w40/Wu_Dynamic_Inference_A_New_Approach_Toward_Efficient_Video_Action_Recognition_CVPRW_2020_paper.html">PDF</a>
                            ]
                            [ <a href="papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf">Slides</a> ]

                            <br>
                        </li>


                        <li><b>Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video
                                Recognition</b> <br>
                            <u><b>Wenhao Wu</b></u>, Dongliang He, Xiao Tan, Shifeng Chen, Shilei Wen
                            <br>
                            <em>
                                <font color="#B71C1C">ICCV 2019</font>&nbsp;&nbsp;<alert>Oral, 4.3% acceptance rate
                                </alert>
                            </em>&nbsp;&nbsp;
                            [ <a
                                href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Multi-Agent_Reinforcement_Learning_Based_Frame_Sampling_for_Effective_Untrimmed_Video_ICCV_2019_paper.pdf">PDF</a>
                            ]
                            [ <a href="papers/ICCV2019/MARL_ICCV19_Poster_Wenhao_Wu.pdf">Poster</a> ]
                            [ <a href="papers/ICCV2019/ICCV19_Oral_5min.pdf">Slides</a> ]

                            <br>
                        </li>


                    </ol><br>






                </ol>
            </div>
        </div>
    </div>
</body>

</html> {% endcomment %}